
http://votchallenge.net/vot2014/results.html
@inproceedings{vot2014,
abstract = {The Visual Object Tracking challenge 2014, VOT2014, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 38 trackers are presented. The number of tested trackers makes VOT 2014 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2014 challenge that go beyond its VOT2013 predecessor are introduced: (i) a new VOT2014 dataset with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2013 evaluation methodology, (iii) a new unit for tracking speed assessment less dependent on the hardware and (iv) the VOT2014 evaluation toolkit that significantly speeds up execution of experiments. The dataset, the evaluation kit as well as the results are publicly available at the challenge website (http://​votchallenge.​net).},
author = {Kristan, Matej and Pflugfelder, Roman and Leonardis, Ale{\v{s}} and Matas, Jiri and {\v{C}}ehovin, Luka and Nebehay, Georg and Voj{\'{i}}ř, Tom{\'{a}}{\v{s}} and Fern{\'{a}}ndez, Gustavo and Luke{\v{z}}i{\v{c}}, Alan and Dimitriev, Aleksandar and Petrosino, Alfredo and Saffari, Amir and Li, Bo and Han, Bohyung and Heng, Cher Keng and Garcia, Christophe and Panger{\v{s}}i{\v{c}}, Dominik and H{\"{a}}ger, Gustav and Khan, Fahad Shahbaz and Oven, Franci and Possegger, Horst and Bischof, Horst and Nam, Hyeonseob and Zhu, Jianke and Li, Ji Jia and Choi, Jin Young and Choi, Jin Woo and Henriques, Jo{\~{a}}o F. and van de Weijer, Joost and Batista, Jorge and Lebeda, Karel and {\"{O}}fj{\"{a}}ll, Kristoffer and Yi, Kwang Moo and Qin, Lei and Wen, Longyin and Maresca, Mario Edoardo and Danelljan, Martin and Felsberg, Michael and Cheng, Ming Ming and Torr, Philip and Huang, Qingming and Bowden, Richard and Hare, Sam and Lim, Samantha Yue Ying and Hong, Seunghoon and Liao, Shengcai and Hadfield, Simon and Li, Stan Z. and Duffner, Stefan and Golodetz, Stuart and Mauthner, Thomas and Vineet, Vibhav and Lin, Weiyao and Li, Yang and Qi, Yuankai and Lei, Zhen and Niu, Zhi Heng},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-319-16181-5_14},
isbn = {9783319161808},
issn = {16113349},
keywords = {Performance evaluation,Short-term single-object trackers,VOT},
pages = {191--217},
title = {{The Visual Object Tracking VOT2014 Challenge Results}},
volume = {8926},
year = {2015}
}



@inproceedings{50seqs,
	author       = "Yi Wu and Jongwoo Lim and Ming-Hsuan Yang",
	title        = "Online Object Tracking: A Benchmark",
	booktitle    = "IEEE Conference on Computer Vision and Pattern Recognition",
	year         = "2013",
	pages        = "2411-2418"
}

@article{kcf,
	author = {Jo{\~a}o F. Henriques and Rui Caseiro and Pedro Martins and Jorge Batista},
	title = {High-Speed Tracking with Kernelized Correlation Filters},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	year = 2015,
	doi = {10.1109/TPAMI.2014.2345390}
}

@inproceedings{vtd,
	author    = {Junseok Kwon and Kyoung Mu Lee},
	title     = {Visual Tracking Decomposition},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year      = {2010},
	pages     = {1269-1276}
}

@inproceedings{kcfdp,
	author    = {Dafei Huang and Lei Luo and Mei Wen and Zhaoyun Chen and Chunyuan Zhang},
	title     = {Enable Scale and Aspect Ratio Adaptability in Visual Tracking with Detection Proposals},
	booktitle = {BMVC},
	year      = {2015}
}

@inproceedings{act,
	author = {Danelljan, Martin and Shahbaz Khan, Fahad and Felsberg, Michael and Van de Weijer, Joost},
	title = {Adaptive Color Attributes for Real-Time Visual Tracking},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year = {2014},
	pages = {1090-1097}
}

@inproceedings{scm,
	author = {Wei Zhong and Huchuan Lu and Ming-Hsuan Yang},
	title = {Robust Object Tracking via Sparsity-Based Collaborative Model},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year = {2012},
	pages = {1838-1845}
}

@inproceedings{lsk,
	author = {Baiyang Liu and Junzhou Huang and Lin Yang and Casimir Kulikowsk},
	title = {Robust Tracking Using Local Sparse Appearance Model and K-Selection},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year = {2011},
	pages = {1313-1320}
}

@inproceedings{asla,
	author = {Xu Jia and Huchuan Lu and Ming-Hsuan Yang},
	title = {Visual Tracking via Adaptive Structural Local Sparse Appearance Model},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year = {2012},
	pages = {1822-1829}
}

@inproceedings{stc,
	author = {Kaihua Zhang and Lei Zhang and David Zhang and Ming-Hsuan Yang},
	title = {Fast Visual Tracking via Dense Spatio-Temporal Context Learning},
	booktitle = {European Conference on Computer Vision},
	year = {2014},
	pages = {127-141}
}

@inproceedings{pbcf,
	author = {Ting Liu and Gang Wang and Qingxiong Yang},
	title = {Real-Time Part-Based Visual Tracking via Adaptive Correlation Filters},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year = {2015},
	pages = {4902-4912}
}

@article{colornaming,
	author = {Van de Weijer, Joost and Cordelia Schmid and Jakob Verbeek and Diane Larlus},
	title = {Learning Color Names for Real-World Applications},
	journal = {IEEE Transactions on Image Processing},
	volume = 18, 
	number = 7, 
	pages = {1512-1523}, 
	year = 2009
}

@inproceedings{samf,
	author = {Yang Li and Jianke Zhu},
	title = {A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration},
	booktitle = {European Conference on Computer Vision Workshop},
	year = {2014},
	pages = {254-265}
}

@article{colorhistogram,
	author = {Dorin Comaniciu and Visvanathan Ramesh and Peter Meer},
	title = {Kernel-based object tracking},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume = 25, 
	number = 5, 
	pages = {564-577}, 
	year = 2003
}

@inproceedings{hog,
	author = {Navneet Dalal and Bill Triggs},
	title = {Histograms of Oriented Gradients for Human Detection},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year = {2005},
	pages = {886-893}
}

@inproceedings{dsst,
abstract = {Robust scale estimation is a challenging problem in visual object tracking. Most existing methods fail to handle large scale variations in complex image sequences. This paper presents a novel approach for robust scale estimation in a tracking-by-detection framework. The proposed approach works by learning discriminative correlation filters based on a scale pyramid representation. We learn separate filters for translation and scale estimation, and show that this improves the performance compared to an exhaustive scale search. Our scale estimation approach is generic as it can be incorporated into any tracking method with no inherent scale estimation. Experiments are performed on 28 benchmark sequences with significant scale vari-ations. Our results show that the proposed approach significantly improves the perfor-mance by 18.8{\%} in median distance precision compared to our baseline. Finally, we provide both quantitative and qualitative comparison of our approach with state-of-the-art trackers in literature. The proposed method is shown to outperform the best existing tracker by 16.6{\%} in median distance precision, while operating at real-time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4290v2},
author = {Danelljan, Martin and H{\"{a}}ger, Gustav and Felsberg, Michael},
booktitle = {British Machine Vision Conference},
doi = {10.1017/CCOL0521824737.025},
eprint = {arXiv:1401.4290v2},
isbn = {0521824737},
issn = {0014-2956},
pmid = {10102984},
title = {{Accurate Scale Estimation for Robust Visual Tracking}},
year = {2014}
}

@inproceedings{plt,
abstract = {We present a quantitative evaluation of Matrioska, a novel framework for the detection and tracking in real-time of unknown object in a video stream, on the LTDT2014 dataset that includes six sequences for the evaluation of single-object long-term visual trackers. Matrioska follows the approach of tracking by detection: the detector localizes the target object in each frame, using multiple keypoint-based methods. To account for appearance changes, the learning module updates both the target object and background model with a growing and pruning approach.},
author = {Maresca, Mario Edoardo and Petrosino, Alfredo},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.128},
isbn = {9781479943098},
issn = {21607516},
pages = {720--725},
title = {{The Matrioska Tracking Algorithm on LTDT2014 Dataset}},
year = {2014}
}



@inproceedings{tld,
	author = {Zdenek Kalal and Jiri Matas and Krystian Mikolajczyk},
	title = {{P-N} Learning: Bootstrapping Binary Classifiers by Structural Constraints},
	booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	year = {2010},
	pages = {49--56}
}

@article{tldjournal,
abstract = {This paper investigates long-term tracking of unknown objects in a video stream. The object is defined by its location and extent in a single frame. In every frame that follows, the task is to determine the object's location and extent or indicate that the object is not present. We propose a novel tracking framework (TLD) that explicitly decomposes the long-term tracking task into tracking, learning and detection. The tracker follows the object from frame to frame. The detector localizes all appearances that have been observed so far and corrects the tracker if necessary. The learning estimates detector's errors and updates it to avoid these errors in the future. We study how to identify detector's errors and learn from them. We develop a novel learning method (P-N learning) which estimates the errors by a pair of "experts'': (i) P-expert estimates missed detections, and (ii) N-expert estimates false alarms. The learning process is modeled as a discrete dynamical system and the conditions under which the learning guarantees improvement are found. We describe our real-time implementation of the TLD framework and the P-N learning. We carry out an extensive quantitative evaluation which shows a significant improvement over state-of-the-art approaches.},
author = {Kalal, Zdenek and Mikolajczyk, Krystian and Matas, Jiri},
doi = {10.1109/TPAMI.2011.239},
isbn = {2011030153},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Long-term tracking,bootstrapping,learning from video,real time,semi-supervised learning},
number = {7},
pages = {1409--1422},
pmid = {22156098},
title = {{Tracking-Learning-Detection}},
volume = {34},
year = {2012}
}



@inproceedings{struck,
	author = {Sam Hare and Amir Saffari and Philip H. S. Torr},
	title = {Struck: Structured Output Tracking with Kernels},
	booktitle = {International Conference on Computer Vision},
	year = {2011},
	pages = {263-270}
}

@inproceedings{csk,
	author = {Jo{\~a}o F. Henriques and Rui Caseiro and Pedro Martins and Jorge Batista},
	title = {Exploiting the Circulant Structure of Tracking-by-Detection with Kernels},
	booktitle = {European Conference on Computer Vision},
	year = {2012},
	pages = {702-715}
}

@inproceedings{mosse,
	author = {David S. Bolme and J. Ross Beveridge and Bruce A. Draper and Yui Man Lui},
	title = {Visual Object Tracking using Adaptive Correlation Filters},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year = {2010},
	pages = {2544-2550}
}

@inproceedings{rcnn,
	Author    = {Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
	Title     = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
	Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	Year      = {2014},
	pages = {580-587}
}

@inproceedings{spp,
	Author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	Title     = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
	Booktitle = {European Conference on Computer Vision},
	Year      = {2014},
	pages = {346-361}
}

@misc{PMT, 
	author = {Piotr Doll\'ar}, 
	title = {{P}iotr's {C}omputer {V}ision {M}atlab {T}oolbox ({PMT})}, 
	howpublished = {\small\url{http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html}} 
}

@online{bingImpl, 
	author = {Tianfei Zhou}, 
	title = {BING Objectness proposal estimator Matlab (mex-c) wrapper}, 
	url = {https://github.com/tfzhou/BINGObjectness},
	year = {2015}
}

@Article{voc2007, 
	author = "Everingham, M. and Eslami, S. M. A. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.", 
	title = "The Pascal Visual Object Classes Challenge: {a retrospective}", 
	journal = "IJCV", 
	volume = "111", 
	year = "2015", 
	number = "1",  
	pages = "98-136", 
} 


@inproceedings{edgeboxes,
	author    = {C. Lawrence Zitnick and Piotr Doll\'ar},
	title     = {{Edge Boxes}: Locating Object Proposals from Edges},
	booktitle = {European Conference on Computer Vision},
	year      = {2014},
	pages = {391-405}
}

@article{selectivesearch,
	author = {Jasper R. R. Uijlings and Koen E. A. van de Sande and Theo Gevers and Arnold W. M. Smeulders},
	title = {Selective Search for Object Recognition},
	journal = {IJCV},
	volume = 104, 
	number = 2, 
	pages = {154-171}, 
	year = 2013
}

@inproceedings {mcg,
	title = {Multiscale Combinatorial Grouping},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year = {2014},
	author = {Pablo Arbelaez and Pont-Tuset, J. and Barron, Jon and Marqu{\'e}s, F. and Jitendra Malik},
	pages = {328-335}
}


@article{cpmc,
	author = {Jo{\~a}o Carreira and Cristian Sminchisescu },
	title = {{CPMC}: Automatic Object Segmentation Using Constrained Parametric Min-Cuts},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume = 34, 
	number = 7, 
	pages = {1312-1328}, 
	year = 2012
}


@inproceedings{geodesic,
	title={Geodesic Object Proposals},
	author={Philipp Kr{\"a}henb{\"u}hl and Vladlen Koltun},
	booktitle={European Conference on Computer Vision},
	year={2014},
	pages = {725-739}
}

@article{objectness,
	author = {Alexe, Bogdan and Deselaers, Thomas and Ferrari, Vittorio},
	title = {Measuring the Objectness of Image Windows},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume = 34, 
	number = 11, 
	pages = {2189-2202}, 
	year = 2012
}

@inproceedings{bing,
	title={{BING}: Binarized Normed Gradients for Objectness Estimation at 300fps},
	author={Ming-Ming Cheng and Ziming Zhang and Wen-Yan Lin and Philip H. S. Torr},
	booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
	year={2014},
	pages = {3286-3293}
}

@article{dpsurvey,
	author = {Jan Hosang and Rodrigo Benenson and Piotr Doll\'ar and Bernt Schiele},
	title = {What Makes for Effective Detection Proposals?},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	year = 2015,
	doi = {10.1109/TPAMI.2015.2465908}
}


@inproceedings{dpcompare,
	author = {J. Hosang and R. Benenson and B. Schiele},
	title = {How Good are Detection Proposals, Really?},
	booktitle = {BMVC},
	year = {2014}
}

@inproceedings{structurededge,
	author = {Piotr Doll\'ar and C. Lawrence Zitnick},
	title = {Structured Forests for Fast Edge Detection},
	booktitle = {International Conference on Computer Vision},
	year = {2013},
	pages = {1841-1848}
}

@inproceedings{decitree,
	author    = {Wang, Aiping and Wan, Guowei and Cheng, Zhiquan and Li, Sikun},
	title     = {An incremental extremely random forest classifier for online learning and tracking},
	booktitle = {ICIP},
	year      = {2009},
	pages = {1449-1452}
}


@article{multicue,
	author = {Wang, Aiping and Cheng, Zhiquan and Martin, Ralph R. and Li, Sikun},
	title = {Multiple-cue-based visual object contour tracking with incremental learning},
	journal = {LNCS},
	volume = 7544, 
	pages = {225-243}, 
	year = 2013
}

@inproceedings{jots,
	author = {Longyin Wen and Dawei Du and Zhen Lei and Stan Z. Li and Ming-Hsuan Yang},
	title = {{JOTS}: Joint Online Tracking and Segmentation},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year      = {2015},
	pages = {2226-2234}
}

@inproceedings{pfseg,
	author = {Vasileios Belagiannis and Falk Schubert and Nassir Navab and Slobodan Ilic},
	title = {Segmentation based particle filtering for real-time 2d object tracking},
	booktitle = {European Conference on Computer Vision},
	year      = {2012},
	pages = {842-855}
}

@inproceedings{pixeltrack,
	author = {Stefan Duffner and Christophe Garcia},
	title = {{PixelTrack}: A Fast Adaptive Algorithm for Tracking Non-Rigid Objects},
	booktitle = {International Conference on Computer Vision},
	year      = {2013},
	pages = {2480-2487}
}

@inproceedings{houghtrack,
	author = {Martin Godec and Peter M. Roth and Horst Bischof},
	title = {Hough-Based Tracking of Non-Rigid Objects},
	booktitle = {International Conference on Computer Vision},
	year      = {2011},
	pages = {81-88}
}

@inproceedings{vot2013,
	author = {Matej Kristan and Roman Pflugfelder and Ales Leonardis and {\em et al}},
	title = {The Visual Object Tracking {VOT2013} Challenge Results},
	booktitle = {IEEE International Conference on Computer Vision Workshop},
	year      = {2013},
	pages = {98-111}
}

@inproceedings{vot2015,
abstract = {The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 pre-decessor are: (i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website 1 .},
author = {Kristan, Matej and Pflugfelder, Roman and Leonardis, Ale{\v{s}} and Matas, Jiri and {\v{C}}ehovin, Luka and Nebehay, Georg and Voj{\'{i}}$\backslash$vr, Tom{\'{a}}{\v{s}} and Fernandez, Gustavo and Others},
booktitle = {IEEE International Conference on Computer Vision Workshop},
doi = {10.1109/ICCVW.2015.79},
isbn = {9780769557205},
issn = {16113349},
pages = {1--27},
title = {{The Visual Object Tracking {VOT2015} Challenge Results}},
year = {2015}
}

@inproceedings{mdnet,
abstract = {We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify target in each domain. We train each domain in the network iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance in existing tracking benchmarks.},
archivePrefix = {arXiv},
arxivId = {1510.07945},
author = {Nam, Hyeonseob and Han, Bohyung},
booktitle = {arXiv preprint arXiv:1510.07945},
doi = {10.1109/CVPR.2016.465},
eprint = {1510.07945},
isbn = {9781467388511},
issn = {10636919},
pages = {4293--4302},
title = {{Learning Multi-Domain Convolutional Neural Networks for Visual Tracking}},
url = {http://arxiv.org/abs/1510.07945},
year = {2015}
}


@inproceedings{dgt,
	author = {Cai, Z. and Wen, L. and Yang, J. and Lei, Z. and Li, S.Z.},
	title = {Structured visual tracking with dynamic graph},
	booktitle = {ACCV},
	year      = {2012},
	pages = {86-97}
}

@inproceedings{proposalSelect,
	author = {Yang Hua and Karteek Alahari and Cordelia Schmid},
	title = {Online Object Tracking with Proposal Selection},
	booktitle = {International Conference on Computer Vision},
	year      = {2015},
	pages = {3092-3100}
}

@article{adobing,
	author = {Pengpeng Liang and Yu Pang and Chunyuan Liao and Xue Mei and Haibin Ling},
	title = {Adaptive Objectness for Object Tracking},
	journal = {IEEE Signal Processing Letters},
	volume = 23, 
	number = 7, 
	pages = {949-953}, 
	year = 2016
}

@inproceedings{moca,
	author = {Guibo	Zhu and Jinqiao Wang and Yi Wu and Xiaoyu Zhang and Hanqing Lu},
	title = {{MC-HOG} Correlation Tracking with Saliency Proposal},
	booktitle = {AAAI},
	year      = {2016},
	pages = {3690-3696}
}

@inproceedings{ebt,
	author = {Gao Zhu and Fatih Porikli and Hongdong Li},
	title = {Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
	year      = {2016},
	pages = {943-951}
}

@inproceedings{rpnt,
	author = {Gao Zhu and Fatih Porikli and Hongdong Li},
	title = {Robust Visual Tracking with Deep Convolutional Neural Network based Object Proposals on {PETS}},
	booktitle = {IEEE Conference on Computer Vision and Pattern Recognition Workshop},
	year      = {2016},
	pages = {26-33}
}

@inproceedings{rpn,
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	title = {Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	booktitle = {NIPS},
	year      = {2015},
	pages = {91-99}
}



@misc{Authors06,
 author = {Authors},
 title = {The frobnicatable foo filter},
 note = {ECCV06 submission ID 324. Supplied as additional material {\tt eccv06.pdf}},
 year = 2006
}

@misc{Authors06b,
 author = {Authors},
 title = {Frobnication tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2006
}

@article{Alpher02,
author = {A. Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {A. Alpher and J.~P.~N. Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {A. Alpher and J.~P.~N. Fotheringham-Smythe and G. Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}

@article{Mermin89,
author = {N. David Mermin},
title = {What's wrong with these equations?},
journal = {Physics Today},
year = 1989,
month = oct,
note = {\small\url{http://www.cvpr.org/doc/mermin.pdf}}
}

@Book{Hartley00,
  author       = "Hartley, R.~I. and Zisserman, A.",
  title        = "Multiple View Geometry in Computer Vision",
  year         = "2000",
  publisher    = "Cambridge University Press, ISBN: 0521623049",
}

@InProceedings{Claus05,
  author       = "Claus, D. and Fitzgibbon, A.~W.",
  title        = "A Rational Function Lens Distortion Model for General
                 Cameras",
  booktitle    = "Proc. CVPR",
  year         = "2005",
  pages        = "213--219",
}

@InProceedings{Zhang96,
  author =	 "Z. Zhang",
  title =	 "On the Epipolar Geometry Between Two Images With
                  Lens Distortion",
  booktitle =	 "Proc. ICPR",
  year =	 "1996",
  pages =	 "407--411",
}

@InProceedings{Fitzgibbon01,
  author       = "Fitzgibbon, A.~W.",
  title        = "Simultaneous Linear Estimation of Multiple View
                 Geometry and Lens Distortion",
  booktitle    = "Proc. CVPR",
  year         = "2001"
}

@article{Brown71,
author = {D.~C. Brown},
title = {Close-range camera calibration},
journal = {Photogrammetric Eng.},
volume = 37,
number = 8,
pages = {855--866}, 
year = 1971
}

@article{Devernay01,
author = {F. Devernay and O. Faugeras},
title = {Straight lines have to be straight},
journal = {MVA},
volume = 13,
pages = {14--24}, 
year = 2001
}

@article{Swaminathan00,
author = {R. Swaminathan and S. Nayar},
title = {Nonmetric calibration of wide-angle lenses and polycameras},
journal = {IEEE T-PAMI},
volume = 22,
number = 10,
pages = {1172--1178}, 
year = 2000
}

@InProceedings{Tsai86,
  author =	 "Tsai, Y.~R.",
  title =	 "An Efficient and Accurate Camera Calibration
                  Technique for {3D} Machine Vision",
  booktitle =	 "Proc. CVPR",
  year =	 "1986",
}


@inproceedings{jetson,
author = {Stone, John E. and Hallock, Michael J. and Phillips, James C. and Peterson, Joseph R. and Luthey-Schulten, Zaida and Schulten, Klaus},
booktitle = {IEEE 28th International Parallel and Distributed Processing Symposium Workshops},
doi = {10.1109/IPDPSW.2016.130},
isbn = {9781509021406},
keywords = {Energy efficiency,GPU computing,Heterogeneous architectures,High-performance computing,Mobile computing,Molecular modeling},
pages = {89--100},
title = {{Evaluation of Emerging Energy-Efficient Heterogeneous Computing Platforms for Biomolecular and Cellular Simulation Workloads}},
volume = {2016-August},
year = {2016}
}

@misc{top500,
	lab = {TOP500.org},
	author = {TOP500},
	organization = {TOP500.org},
	year =         {2010},
	title =        {{TOP500} lists: November 2010},
	url =          {https://www.top500.org/lists/2010/11/highlights/}
}

@misc{top500th2,
	lab = {TOP500.org},
	author = {TOP500},
	organization = {TOP500.org},
	year =         {2014},
	title =        {{TOP500} lists: November 2014},
	url =          {http://top500.org/lists/2014/11/}
}

@book{oclspec,
author = {Munshi, A},
institution = {Khronos OpenCL Working Group and others},
title = {{The {OpenCL} Specification}},
url = {http://www.khronos.org/opencl},
year = {2011}
}

@inproceedings{fasterrcnn,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Conference on Neural Information Processing Systems},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
isbn = {0162-8828 VO - PP},
issn = {01689002},
pages = {1--10},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}

@inproceedings{fastrcnn,
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
isbn = {9781467383912},
issn = {15505499},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
year = {2015}
}

@misc{openmp,
	organization = {OpenMP Architecture Review Board},
	year =         {2015},
	title =        {Open Multi-Processing},
	url =          {http://www.openmp.org/}
}

@book{cudaspec,
author = {{NVIDIA Corporation}},
keywords = {Nvidia},
title = {{CUDA C Programming Guide}},
url = {http://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA\_C\_Programming\_Guide.pdf},
year = {2017}
}

@misc{snapdragon,
	organization = {Qualcomm},
	year =         {2017},
	title =        {Snapdragon 835 mobile platform},
	url =          {https://www.qualcomm.com/products/snapdragon/processors/835}
}

@inproceedings{cell,
  title={An {OpenCL} Framework for Heterogeneous Multicores with Local Memory},
  author={J. Lee and J. Kim and S. Seo and S. Kim and J. Park and H. Kim and T. T. Dao and Y. Cho and S. J. Seo and S. H. Lee and others},
  booktitle={Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
  pages={193--204},
  year={2010},
  organization={ACM}
}

@misc{oclppt,
	author = {Tim Mattson and Udeepta Bordoloi},
	year =         {2011},
	title =        {{OpenCL: an Introduction for HPC Programmers}},
	url =          {https://indico.cern.ch/event/138427/sessions/11397/attachments/116552/165428/OpenCL-intro-ISC11.pdf}
}

@inproceedings{lkoptflow,
author = {Lucas, B D and Kanade, T},
booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
keywords = {Image Registration},
pages = {674--679},
title = {{An Iterative Image Registration Technique with an Application to Stereo Vision}},
volume = {2},
year = {1981}
}

@inproceedings{lkoptflow2,
author = {Shi, Jianbo and Tomasi, Carlo},
booktitle = {Proceedings of  the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {593--600},
title = {{Good Features to Track}},
year = {1994}
}

@article{ncc,
author = {Lewis, J},
journal = {Vision Interface},
pages = {120--123},
title = {{Fast Normalized Cross-Correlation}},
volume = {10},
year = {1995}
}

@inproceedings{fern,
abstract = {While feature point recognition is a key component of modern approaches to object detection, existing approaches require computationally expensive patch preprocessing to handle perspective distortion. In this paper, we show that formulating the problem in a Naive Bayesian classification framework makes such preprocessing unnecessary and pro-duces an algorithm that is simple, efficient, and robust. Fur-thermore, it scales well to handle large number of classes. To recognize the patches surrounding keypoints, our classifier uses hundreds of simple binary features and mod-els class posterior probabilities. We make the problem com-putationally tractable by assuming independence between arbitrary sets of features. Even though this is not strictly true, we demonstrate that our classifier nevertheless per-forms remarkably well on image datasets containing very significant perspective changes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5736v1},
author = {{\"{O}}zuysal, Mustafa and Fua, Pascal and Lepetit, Vincent},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383123},
eprint = {arXiv:1407.5736v1},
isbn = {1424411807},
issn = {10636919},
pmid = {21736739},
title = {{Fast Keypoint Recognition in Ten Lines of Code}},
year = {2007}
}

@misc{opentld,
	author = {Zdenek Kalal},
	year =         {2011},
	title =        {{OpenTLD}},
	url =          {https://github.com/zk00006/OpenTLD}
}

@article{htld,
abstract = {{\textcopyright} 2015 Springer-Verlag Berlin HeidelbergThe recently proposed tracking–learning–detection (TLD) method has become a popular visual tracking algorithm as it was shown to provide promising long-term tracking results. On the other hand, the high computational cost of the algorithm prevents it being used at higher resolutions and frame rates. In this paper, we describe the design and implementation of a heterogeneous CPU–GPU TLD (H-TLD) solution using OpenMP and CUDA. Leveraging the advantages of the heterogeneous architecture, serial parts are run asynchronously on the CPU while the most computationally costly parts are parallelized and run on the GPU. Design of the solution ensures keeping data transfers between CPU and GPU at a minimum and applying stream compaction and overlapping data transfer with computation whenever such transfers are necessary. The workload is balanced for a uniform work distribution across the GPU multiprocessors. Results show that 10.25 times speed-up is achieved at 1920 (Formula presented.) 1080 resolution compared to the baseline TLD. The source code has been made publicly available to download from the following address: http://gpuresearch.ii.metu.edu.tr/codes/.},
author = {Gurcan, Ilker and Temizel, Alptekin},
doi = {10.1007/s11554-015-0538-y},
issn = {18618200},
journal = {Journal of Real-Time Image Processing},
keywords = {CUDA,Heterogeneous CPU???GPU implementations,Object tracking,Real time},
pages = {1--15},
title = {{Heterogeneous CPU-GPU Tracking-Learning-Detection (H-TLD) for Real-Time Object Tracking}},
year = {2015}
}

@misc{opencv,
abstract = {Open Source Computer Vision is a library of programming functions for real time computer vision},
organization = {Intel Corporation, Willow Garage, Itseez},
title = {{OpenCV Library}},
url = {http://opencv.org/},
year = {2016}
}

@misc{intelocl,
organization = {Intel Corporation},
title = {{OpenCL Drivers and Runtimes for Intel Architecture}},
url = {https://software.intel.com/en-us/articles/opencl-drivers},
year = {2016}
}

@techreport{mic,
organization = {Intel Corporation},
number = {328209 004EN},
title = {{Intel Xeon Phi Coprocessor x100 Product Family}},
year = {2015}
}



@inproceedings{affineloop,
  title={A Compiler Framework for Optimization of Affine Loop Nests for {GPGPUs}},
  author={M. M. Baskaran and U. Bondhugula and S. Krishnamoorthy and J. Ramanujam and A. Rountev and P. Sadayappan},
  booktitle={Proceedings of the 22nd annual international conference on Supercomputing},
  pages={225--234},
  year={2008},
  organization={ACM}
}

@techreport{TReporthIllinois,
  title={Performance Portability in Accelerated Parallel Kernels},
  author={J. A. Stratton and H. Kim and T. B. Jablin and W. M. W. Hwu},
  journal={Center for Reliable and High-Performance Computing},
  institution = {University of Illinois at Urbana-Champaign},
  year={2013},
  number = {IMPACT-13-01},
}

@article{OclPP1,
  title={{An} Investigation of the Performance Portability of {OpenCL}},
  author={S. J. Pennycook and S. D. Hammond and S. A. Wright and J. A. Herdman and I. Miller and S. A. Jarvis},
  journal={Journal of Parallel and Distributed Computing},
  volume={73},
  number={11},
  pages={1439--1450},
  year={2013},
  publisher={Elsevier}
}

@inproceedings{OclPP2,
  title={An Experimental Study on Performance Portability of {OpenCL} Kernels},
  author={S. Rul and H. Vandierendonck and J. D'Haene and K. De Bosschere},
  booktitle={Proceedings of the 2010 Symposium on Application Accelerators in High Performance Computing},
  year={2010}
  location     = {Knoxville, TN, USA},
}

@inproceedings{OclPP3,
  title={Cross-Platform {OpenCL} Code and Performance Portability for {CPU} and {GPU} Architectures Investigated with a Climate and Weather Physics Model},
  author={H. Dong and D. Ghosh and F. Zafar and S. Zhou},
  booktitle={Proceedings of the 41st International Conference on Parallel Processing Workshops},
  pages={126--134},
  year={2012},
  organization={IEEE}
  address = {Pittsburgh, PA, USA},
}

@inproceedings{cell,
  title={An {OpenCL} Framework for Heterogeneous Multicores with Local Memory},
  author={J. Lee and J. Kim and S. Seo and S. Kim and J. Park and H. Kim and T. T. Dao and Y. Cho and S. J. Seo and S. H. Lee and others},
  booktitle={Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
  pages={193--204},
  year={2010},
  organization={ACM}
  address = {Vienna, Austria},
}

@inproceedings{mcuda,
  title={{MCUDA}: An Effective Implementation of {CUDA} Kernels for Multi-Core {CPUs}},
  author={J. A. Stratton and S. S. Stone and W. M. W. Hwu},
  booktitle={Proceedings of the 21st International Workshop on Languages and Compilers for Parallel Computing},
  pages={16--30},
  year={2008},
  publisher={Springer}
}

@inproceedings{efficient,
  title={Efficient Compilation of Fine-Grained {SPMD} Threaded Programs for Multicore {CPUs}},
  author={J. A. Stratton and V. Grover and J. Marathe and B. Aarts and M. Murphy and Z. Hu and W. M. W. Hwu},
  booktitle={Proceedings of the 8th annual IEEE/ACM International Symposium on Code Generation and Optimization},
  pages={111--119},
  year={2010},
  organization={ACM}
  address = {Toronto, Ontario, Canada},
}

@inproceedings{twinpeaks,
  title={Twin Peaks: a Software Platform for Heterogeneous Computing on General-Purpose and Graphics Processors},
  author={J. Gummaraju and L. Morichetti and M. Houston and B. Sander and B. R. Gaster and B. Zheng},
  booktitle={Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
  pages={205--216},
  year={2010},
  organization={ACM}
}


@misc{inteloclguide,
  author =       {{Intel Corporation}},
  organization = {Intel Corporation},
  lab = {Intel Corporation},
  year =         {2013},
  title =        {Intel {SDK} for {OpenCL} Applications {XE} 2013 Optimization Guide},
}

@misc{intelsdk,
  author =       {{Intel Corporation}},
  organization = {Intel Corporation},
  lab = {Intel Corporation},
  year =         {2013},
  title =        {{Intel SDK for OpenCL Applications}},
  url = {https://software.intel.com/en-us/intel-opencl}
}


@article{autotuning1,
  title={{From} {CUDA} to {OpenCL}: Towards a Performance-Portable Solution for Multi-Platform {GPU} Programming},
  author={P. Du and R. Weber and P. Luszczek and S. Tomov and G. Peterson and J. Dongarra},
  journal={Parallel Computing},
  volume={38},
  number={8},
  pages={391--407},
  year={2012},
  publisher={Elsevier}
}


@inproceedings{asplos,
  title={Portable Performance on Heterogeneous Architectures},
  author={P. M. Phothilimthana and J. Ansel and J. Ragan-Kelley and S. Amarasinghe},
  booktitle={Proceedings of the 18th International Conference on Architechtural Support for Programming Languages and Operating Systems},
  volume={48},
  number={4},
  pages={431--444},
  year={2013},
  organization={ACM}
  address = {Houston, Texas, USA},
}

@article{shen,
  title={{An} Empirical Study of {Fortran} Programs for Parallelizing Compilers},
  author={Z. Shen and Z. Li and P. Yew},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={1},
  number={3},
  pages={356--364},
  year={1990},
  publisher={IEEE}
}

@article{paek,
  title={{Efficient} and Precise Array Access Analysis},
  author={Y. Paek and J. Hoeflinger and D. Padua},
  journal={ACM Transactions on Programming Languages and Systems},
  volume={24},
  number={1},
  pages={65--109},
  year={2002},
  publisher={ACM}
}

@inproceedings{triolet,
  title={Direct Parallelization of Call Statements},
  author={R. Triolet and F. Irigoin and P. Feautrier},
  booktitle={ACM SIGPLAN Notices},
  volume={21},
  number={7},
  pages={176--185},
  year={1986},
  organization={ACM}
}

@inproceedings{bala,
  title={A Technique for Summarizing Data Access and its Use in Parallelism Enhancing Transformations},
  author={V. Balasundaram and K. Kennedy},
  volume={24},
  number={7},
  year={1989},
  publisher={ACM},
  booktitle = {Proceedings of the {ACM SIGPLAN} 1989 Conference on Programming Language Design and Implementation},
  pages = {41--53}
}

@inproceedings{pmodel,
  title={Code Generation in the Polyhedral Model is Easier than You Think},
  author={C. Bastoul},
  booktitle={Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques},
  pages={7--16},
  year={2004},
  organization={IEEE Computer Society}
}




@misc{oclprogrammingguide,
  author={{Nvidia Corporation}},
  organization={Nvidia Corporation},
  lab={Nvidia Corporation},
  year={2011},
  title={{OpenCL} Programming Guide for the {CUDA} Architecture},
}

@misc{oclbestpractice,
	author={{Nvidia Corporation}},
	organization={Nvidia Corporation},
	lab={Nvidia Corporation},
	year={2011},
	title={{OpenCL} Best Practices Guide},
}

@book{compilerbook,
  title={Optimizing Compilers for Modern Architectures: A Dependence-Based Approach},
  author={R. Allen and K. Kennedy},
  year={2002},
  publisher={Morgan Kaufmann San Francisco}
}

@book{compilerbook2,
  title={Advanced Compiler Design and Implementation},
  author={S. M. Steven},
  year={1997},
  publisher={Morgan Kaufmann}
}


@misc{intelvectorguide,
  author = {{Intel Corporation}},
  organization = {Intel Corporation},
  lab = {Intel Corporation},
  year =         {2012},
  title =        {A Guide to Vectorization with {Intel} {C++} Compilers},
}



@misc{clang,
  title={Clang: A {C} Language Family Frontend for {LLVM}},
  author={{LLVM Team and others}},
  lab = {LLVM Team and others},
  organization={LLVM Team and others},
  year={2012},
  url={http://clang.llvm.org/},
}

@inbook{llvm,
  title={The {LLVM} Compiler Framework and Infrastructure Tutorial},
  author={C. Lattner and V. Adve},
  booktitle={Languages and Compilers for High Performance Computing},
  pages={15--16},
  year={2005},
  volume={},
  number={},
  publisher={Springer}
}


@misc{intelintrin,
  author = {Intel},
  lab = {Intel Corporation},
  organization = {Intel Corporation},
  title = {{Intel} {C++} Intrinsic Reference},
  year = {2013},
}


@misc{freeocl,
  lab = {{FreeOCL}},
  author = {{FreeOCL}},
  organization = {{FreeOCL}},
  year =         {2012},
  title =        {{FreeOCL}: Multi-Platform Implementation of {OpenCL} 1.2 Targeting {CPUs}},
  url =          {https://code.google.com/p/freeocl},
}


@inproceedings{shoc,
  title={The Scalable HeterOgeneous Computing ({SHOC}) Benchmark Suite},
  author={A. Danalis and G. Marin and C. McCurdy and J. S. Meredith and P. C. Roth and K. Spafford and V. Tipparaju and J. S. Vetter},
  booktitle={Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units},
  pages={63--74},
  year={2010},
  organization={ACM}
  address = {Pittsburgh, PA, USA},
}

@inproceedings{burke86,
  title={Interprocedural Dependence Analysis and Parallelization},
  author={M. Burke and R. Cytron},
  booktitle={ACM Sigplan Notices},
  volume={21},
  number={7},
  pages={162--175},
  year={1986},
  organization={ACM},
}

@article{aristotle,
	title={{Aristotle: A Performance Impact Indicator for the OpenCL Kernels using Local Memory}},
	author={J. Fang and H. Sips and A. L. Varbanescu},
	journal={Scientific Programming},
	volume={22},
	number={3},
	pages={239--257},
	year={2014},
	publisher={Hindawi}
}

@inproceedings{europar14,
	title={Automated Transformation of {GPU}-Specific {OpenCL} Kernels Targeting Performance Portability on Multi-Core/Many-Core {CPU}s},
	author={D. Huang and M. Wen and C. Xun and D. Chen and X. Cai and Y. Qiao and N. Wu and C. Zhang},
	booktitle={Proceedings of Euro-Par 2014 \& Lecture Notes in Computer Science},
	volume={8632},
	pages={210--221},
	year={2014},
}

@inproceedings{groover,
	title={Grover: Looking for Performance Improvement by Disabling Local Memory Usage in {OpenCL} Kernels},
	author={J. Fang and H. Sips and P. Jaaskelainen and A. L. Varbanescu},
	booktitle={Proceedings of the 43rd International Conference on Parallel Processing},
	pages={162--171},
	year={2014},
	address = {Minneapolis, USA},
}

@article{itpds11,
	title={{Exploiting} Memory Access Patterns to Improve Memory Performance in Data-Parallel Architectures},
	author={B. Jang and Schaa, D. and Mistry, P. and Kaeli, D.},
	journal={ IEEE Transactions on Parallel and Distributed Systems},
	volume={22},
	number={1},
	pages={105--118},
	year={2011},
	publisher={IEEE}
}

@techreport{cean,
author = {{Intel Corporation}},
title = {{CEAN Language Extension and Programming Model}},
url = {https://software.intel.com/sites/default/files/fd/c5/CEAN\_Model.docx}
}

@inproceedings{petabricks,
abstract = {It is often impossible to obtain a one-size-fits-all solution for high performance algorithms when considering different choices for data distributions, parallelism, transformations, and blocking. The best solution to these choices is often tightly coupled to different architectures, problem sizes, data, and available system resources. In some cases, completely different algorithms may provide the best performance. Current compiler and programming language techniques are able to change some of these parameters, but today there is no simple way for the programmer to express or the compiler to choose different algorithms to handle different parts of the data. Existing solutions normally can handle only coarse- grained, library level selections or hand coded cutoffs between base cases and recursive cases. We present PetaBricks, a new implicitly parallel language and compiler where having multiple implementations of multiple algorithms to solve a problem is the natural way of programming. We make algorithmic choice a first class construct of the language. Choices are provided in a way that also allows our compiler to tune at a finer granularity. The PetaBricks compiler autotunes programs by making both fine-grained as well as algorithmic choices. Choices also include different automatic parallelization techniques, data distributions, algorithmic parameters, transformations, and blocking. Additionally, we introduce novel techniques to autotune algorithms for different convergence criteria. When choosing between various direct and iterative methods, the PetaBricks compiler is able to tune a program in such a way that delivers near optimal efficiency for any desired level of accuracy. The compiler has the flexibility of utilizing different convergence criteria for the various components within a single algorithm, providing the user with accuracy choice alongside algorithmic choice.},
author = {Ansel, Jason and Chan, Cy and Wong, Yee Lok and Olszewski, Marek and Zhao, Qin and Edelman, Alan and Amarasinghe, Saman},
booktitle = {Proceedings of the 30th ACM SIGPLAN Conference on Programming Language Design and Implementation},
doi = {10.1145/1543135.1542481},
isbn = {978-1-60558-392-1},
issn = {03621340},
keywords = {C++,autotuning,parallel architecture,parallel benchmarks,petabricks},
number = {6},
pages = {38--49},
title = {{PetaBricks: A Language and Compiler for Algorithmic Choice}},
url = {http://portal.acm.org/citation.cfm?doid=1543135.1542481\%5Cncode.google.com/p/petabricks/},
volume = {44},
year = {2009}
}


@misc{nvidiasdk,
  author = {{NVIDIA Corporation}},
  lab = {NVIDIA Corporation},
  organization = {NVIDIA Corporation},
  title = {{NVIDIA GPU Computing SDK 4.2}},
  year = {2013},
  url = {http://developer.nvidia.com/gpu-computing-sdk}
}



@techreport{klt2,
abstract = {The factorization method described in this series of reports requires an algorithm to track the motion of features in an image stream. Given the small inter-frame displacement made possible by the factorization approach, the best tracking method turns out to be the one proposed by Lucas and Kanade in 1981. The method defines the measure of match between fixed-size feature windows in the past and current frame as the sum of squared intensity differences over the windows. The displacement is then defined as the one that minimizes this sum. For small motions, a linearization of the image intensities leads to a Newton-Raphson style minimization. In this report, after rederiving the method in a physically intuitive way, we answer the crucial question of how to choose the feature windows that are best suited for tracking. Our selection criterion is based directly on the definition of the tracking algorithm, and expresses how well a feature can be tracked. As a result, the criterion is optimal by construction. We show by experiment that the performance of both the selection and the tracking algorithm are adequate for our factorization method, and we address the issue of how to detect occlusions. In the conclusion, we point out specific open questions for future research.},
author = {Tomasi, Carlo and T. Kanade},
booktitle = {School of Computer Science, Carnegie Mellon University},
doi = {10.1016/S0031-3203(03)00234-6},
isbn = {078038234X},
issn = {00313203},
pages = {1--22},
title = {{Detection and Tracking of Point Features}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.131.5899\&rep=rep1\&type=pdf},
volume = {91},
year = {1991},
institution = {School of Computer Science, Carnegie Mellon University},
number = {CMU-CS-91-132}
}

@inproceedings{deepimage,
author = {Wang, Naiyan and D. Y. Yeung},
booktitle = {Annual Conference on Neural Information Processing System},
pages = {809--817},
title = {{Learning a Deep Compact Image Representation for Visual Tracking}},
year = {2013}
}

@inproceedings{deeptrack,
abstract = {Defining hand-crafted feature representations needs expert knowledge, requires time-$\backslash$nconsuming manual adjustments, and besides, it is arguably one of the limiting factors of$\backslash$nobject tracking.$\backslash$n$\backslash$nIn this paper, we propose a novel solution to automatically relearn the most useful$\backslash$nfeature representations during the tracking process in order to accurately adapt appear-$\backslash$nance changes, pose and scale variations while preventing from drift and tracking failures.$\backslash$nWe employ a candidate pool of multiple Convolutional Neural Networks (CNNs) as a$\backslash$ndata-driven model of different instances of the target object. Individually, each CNN$\backslash$nmaintains a specific set of kernels that favourably discriminate object patches from their$\backslash$nsurrounding background using all available low-level cues. These kernels are updated$\backslash$nin an online manner at each frame after being trained with just one instance at the ini-$\backslash$ntialization of the corresponding CNN. Given a frame, the most promising CNNs in the$\backslash$npool are selected to evaluate the hypothesises for the target object. The hypothesis with$\backslash$nthe highest score is assigned as the current detection window and the selected models are$\backslash$nretrained using a warm-start back-propagation which optimizes a structural loss function.$\backslash$nIn addition to the model-free tracker, we introduce a class-specific version of the pro-$\backslash$nposed method that is tailored for tracking of a particular object class such as human faces.$\backslash$nOur experiments on a large selection of videos from the recent benchmarks demonstrate$\backslash$nthat our method outperforms the existing state-of-the-art algorithms and rarely loses the$\backslash$ntrack of the target object.$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1503.0072},
author = {Li, Hanxi and Li, Yi and Porikli, Fatih},
booktitle = {British Machine Vision Conference},
doi = {10.1109/TIP.2015.2510583},
eprint = {1503.0072},
issn = {1057-7149},
pages = {1--12},
title = {{DeepTrack: Learning Discriminative Feature Representations by Convolutional Neural Networks for Visual Tracking}},
year = {2014}
}

@article{survey51,
abstract = {citation: 1540 (2014/07/05)},
author = {Baker, Simon and Matthews, Iain},
doi = {10.1023/B:VISI.0000011205.11775.fd},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {A unifying framework,Additive vs. compositional algorithms,Efficiency,Forwards vs. inverse algorithms,Gauss-Newton,Image alignment,Levenberg-Marquardt,Lucas-Kanade,Newton,Steepest descent,The inverse compositional algorithm},
number = {3},
pages = {221--255},
title = {{Lucas-Kanade 20 Years on: A Unifying Framework}},
volume = {56},
year = {2004}
}

@article{colornaminguse1,
abstract = {In this article we investigate the problem of human action recognition in static images. By action recognition we intend a class of problems which includes both action classification and action detection (i.e. simultaneous localization and classification). Bag-of-words image representations yield promising results for action classification, and deformable part models perform very well object detection. The representations for action recognition typically use only shape cues and ignore color information. Inspired by the recent success of color in image classification and object detection, we investigate the potential of color for action classification and detection in static images. We perform a comprehensive evaluation of color descriptors and fusion approaches for action recognition. Experiments were conducted on the three datasets most used for benchmarking action recognition in still images: Willow, PASCAL VOC 2010 and Stanford-40. Our experiments demonstrate that incorporating color information considerably improves recognition performance, and that a descriptor based on color names outperforms pure color descriptors. Our experiments demonstrate that late fusion of color and shape information outperforms other approaches on action recognition. Finally, we show that the different color–shape fusion approaches result in complementary information and combining them yields state-of-the-art performance for action classification.},
author = {Khan, Fahad Shahbaz and Rao, Muhammad Anwer and {Van De Weijer}, Joost and Bagdanov, Andrew D. and Lopez, Antonio M. and Felsberg, Michael},
doi = {10.1007/s11263-013-0633-0},
issn = {09205691},
journal = {International Journal of Computer Vision},
keywords = {Action recognition,Color features,Image representation},
number = {3},
pages = {205--221},
title = {{Coloring Action Recognition in Still Images}},
volume = {105},
year = {2013}
}

@inproceedings{colornaminguse2,
abstract = {State-of-the-art object detectors typically use shape$\backslash$ninformation as a low level feature representation to capture the$\backslash$nlocal structure of an object. This paper shows that early fusion$\backslash$nof shape and color, as is popular in image classification, leads$\backslash$nto a significant drop in performance for object detection.$\backslash$nMoreover, such approaches also yields suboptimal results for$\backslash$nobject categories with varying importance of color and shape. In$\backslash$nthis paper we propose the use of color attributes as an explicit$\backslash$ncolor representation for object detection. Color attributes are$\backslash$ncompact, computationally efficient, and when combined with$\backslash$ntraditional shape features provide state-of-the-art results for$\backslash$nobject detection. Our method is tested on the PASCAL VOC 2007$\backslash$nand 2009 datasets and results clearly show that our method$\backslash$nimproves over state-of-the-art techniques despite its$\backslash$nsimplicity. We also introduce a new dataset consisting of$\backslash$ncartoon character images in which color plays a pivotal role. On$\backslash$nthis dataset, our approach yields a significant gain of 14{\%} in$\backslash$nmean AP over conventional state-of-the-art methods.},
author = {Khan, Fahad Shahbaz and Anwer, Rao Muhammad and {Van De Weijer}, Joost and Bagdanov, Andrew D. and Vanrell, Maria and Lopez, Antonio M.},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2012.6248068},
isbn = {9781467312264},
issn = {10636919},
pages = {3306--3313},
title = {{Color Attributes for Object Detection}},
year = {2012}
}


@misc{otbweb,
	author = {Yi Wu and Jongwoo Lim and Ming-Hsuan Yang},
	year =         {2015},
	title =        {{Visual Tracker Benchmark v1.0}},
	url =          {http://cvlab.hanyang.ac.kr/tracker_benchmark/benchmark_v10.html}
}

