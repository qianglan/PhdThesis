
http://votchallenge.net/vot2014/results.html
@inproceedings{vot2014,
abstract = {The Visual Object Tracking challenge 2014, VOT2014, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 38 trackers are presented. The number of tested trackers makes VOT 2014 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2014 challenge that go beyond its VOT2013 predecessor are introduced: (i) a new VOT2014 dataset with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2013 evaluation methodology, (iii) a new unit for tracking speed assessment less dependent on the hardware and (iv) the VOT2014 evaluation toolkit that significantly speeds up execution of experiments. The dataset, the evaluation kit as well as the results are publicly available at the challenge website (http://​votchallenge.​net).},
author = {Kristan, Matej and Pflugfelder, Roman and Leonardis, Ale{\v{s}} and Matas, Jiri and {\v{C}}ehovin, Luka and Nebehay, Georg and Voj{\'{i}}ř, Tom{\'{a}}{\v{s}} and Fern{\'{a}}ndez, Gustavo and Luke{\v{z}}i{\v{c}}, Alan and Dimitriev, Aleksandar and Petrosino, Alfredo and Saffari, Amir and Li, Bo and Han, Bohyung and Heng, Cher Keng and Garcia, Christophe and Panger{\v{s}}i{\v{c}}, Dominik and H{\"{a}}ger, Gustav and Khan, Fahad Shahbaz and Oven, Franci and Possegger, Horst and Bischof, Horst and Nam, Hyeonseob and Zhu, Jianke and Li, Ji Jia and Choi, Jin Young and Choi, Jin Woo and Henriques, Jo{\~{a}}o F. and van de Weijer, Joost and Batista, Jorge and Lebeda, Karel and {\"{O}}fj{\"{a}}ll, Kristoffer and Yi, Kwang Moo and Qin, Lei and Wen, Longyin and Maresca, Mario Edoardo and Danelljan, Martin and Felsberg, Michael and Cheng, Ming Ming and Torr, Philip and Huang, Qingming and Bowden, Richard and Hare, Sam and Lim, Samantha Yue Ying and Hong, Seunghoon and Liao, Shengcai and Hadfield, Simon and Li, Stan Z. and Duffner, Stefan and Golodetz, Stuart and Mauthner, Thomas and Vineet, Vibhav and Lin, Weiyao and Li, Yang and Qi, Yuankai and Lei, Zhen and Niu, Zhi Heng},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-16181-5_14},
isbn = {9783319161808},
issn = {16113349},
keywords = {Performance evaluation,Short-term single-object trackers,VOT},
pages = {191--217},
title = {{The Visual Object Tracking VOT2014 challenge results}},
volume = {8926},
year = {2015}
}



@inproceedings{50seqs,
	author       = "Yi Wu and Jongwoo Lim and Ming-Hsuan Yang",
	title        = "Online Object Tracking: A Benchmark",
	booktitle    = "CVPR",
	year         = "2013",
	pages        = "2411-2418"
}

@article{kcf,
	author = {Jo{\~a}o F. Henriques and Rui Caseiro and Pedro Martins and Jorge Batista},
	title = {High-Speed Tracking with Kernelized Correlation Filters},
	journal = {TPAMI},
	year = 2015,
	doi = {10.1109/TPAMI.2014.2345390}
}

@inproceedings{vtd,
	author    = {Junseok Kwon and Kyoung Mu Lee},
	title     = {Visual tracking decomposition},
	booktitle = {CVPR},
	year      = {2010},
	pages     = {1269-1276}
}

@inproceedings{kcfdp,
	author    = {Dafei Huang and Lei Luo and Mei Wen and Zhaoyun Chen and Chunyuan Zhang},
	title     = {Enable Scale and Aspect Ratio Adaptability in Visual Tracking with Detection Proposals},
	booktitle = {BMVC},
	year      = {2015}
}

@inproceedings{act,
	author = {Danelljan, Martin and Shahbaz Khan, Fahad and Felsberg, Michael and Van de Weijer, Joost},
	title = {Adaptive Color Attributes for Real-Time Visual Tracking},
	booktitle = {CVPR},
	year = {2014},
	pages = {1090-1097}
}

@inproceedings{scm,
	author = {Wei Zhong and Huchuan Lu and Ming-Hsuan Yang},
	title = {Robust Object Tracking via Sparsity-based Collaborative Model},
	booktitle = {CVPR},
	year = {2012},
	pages = {1838-1845}
}

@inproceedings{lsk,
	author = {Baiyang Liu and Junzhou Huang and Lin Yang and Casimir Kulikowsk},
	title = {Robust Tracking Using Local Sparse Appearance Model and K-Selection},
	booktitle = {CVPR},
	year = {2011},
	pages = {1313-1320}
}

@inproceedings{asla,
	author = {Xu Jia and Huchuan Lu and Ming-Hsuan Yang},
	title = {Visual Tracking via Adaptive Structural Local Sparse Appearance Model},
	booktitle = {CVPR},
	year = {2012},
	pages = {1822-1829}
}

@inproceedings{stc,
	author = {Kaihua Zhang and Lei Zhang and David Zhang and Ming-Hsuan Yang},
	title = {Fast Visual Tracking via Dense Spatio-Temporal Context Learning},
	booktitle = {ECCV},
	year = {2014},
	pages = {127-141}
}

@inproceedings{pbcf,
	author = {Ting Liu and Gang Wang and Qingxiong Yang},
	title = {Real-time part-based visual tracking via adaptive correlation filters},
	booktitle = {CVPR},
	year = {2015},
	pages = {4902-4912}
}

@article{colornaming,
	author = {Van de Weijer, Joost and Cordelia Schmid and Jakob Verbeek and Diane Larlus},
	title = {Learning color names for real-world applications},
	journal = {TIP},
	volume = 18, 
	number = 7, 
	pages = {1512-1523}, 
	year = 2009
}

@inproceedings{samf,
	author = {Yang Li and Jianke Zhu},
	title = {A Scale Adaptive Kernel Correlation Filter Tracker with Feature Integration},
	booktitle = {ECCV workshop},
	year = {2014},
	pages = {254-265}
}

@article{colorhistogram,
	author = {Dorin Comaniciu and Visvanathan Ramesh and Peter Meer},
	title = {Kernel-based object tracking},
	journal = {TPAMI},
	volume = 25, 
	number = 5, 
	pages = {564-577}, 
	year = 2003
}

@inproceedings{hog,
	author = {Navneet Dalal and Bill Triggs},
	title = {Histograms of Oriented Gradients for Human Detection},
	booktitle = {CVPR},
	year = {2005},
	pages = {886-893}
}

@inproceedings{dsst,
abstract = {Robust scale estimation is a challenging problem in visual object tracking. Most existing methods fail to handle large scale variations in complex image sequences. This paper presents a novel approach for robust scale estimation in a tracking-by-detection framework. The proposed approach works by learning discriminative correlation filters based on a scale pyramid representation. We learn separate filters for translation and scale estimation, and show that this improves the performance compared to an exhaustive scale search. Our scale estimation approach is generic as it can be incorporated into any tracking method with no inherent scale estimation. Experiments are performed on 28 benchmark sequences with significant scale vari-ations. Our results show that the proposed approach significantly improves the perfor-mance by 18.8{\%} in median distance precision compared to our baseline. Finally, we provide both quantitative and qualitative comparison of our approach with state-of-the-art trackers in literature. The proposed method is shown to outperform the best existing tracker by 16.6{\%} in median distance precision, while operating at real-time.},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4290v2},
author = {Danelljan, Martin and H{\"{a}}ger, Gustav and Felsberg, Michael},
booktitle = {Proceedings of the British Machine Vision Conference.},
doi = {10.1017/CCOL0521824737.025},
eprint = {arXiv:1401.4290v2},
isbn = {0521824737},
issn = {0014-2956},
pmid = {10102984},
title = {{Accurate Scale Estimation for Robust Visual Tracking}},
year = {2014}
}

@inproceedings{plt,
abstract = {We present a quantitative evaluation of Matrioska, a novel framework for the detection and tracking in real-time of unknown object in a video stream, on the LTDT2014 dataset that includes six sequences for the evaluation of single-object long-term visual trackers. Matrioska follows the approach of tracking by detection: the detector localizes the target object in each frame, using multiple keypoint-based methods. To account for appearance changes, the learning module updates both the target object and background model with a growing and pruning approach.},
author = {Maresca, Mario Edoardo and Petrosino, Alfredo},
booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
doi = {10.1109/CVPRW.2014.128},
isbn = {9781479943098},
issn = {21607516},
pages = {720--725},
title = {{The matrioska tracking algorithm on LTDT2014 dataset}},
year = {2014}
}



@inproceedings{tld,
	author = {Zdenek Kalal and Jiri Matas and Krystian Mikolajczyk},
	title = {{P-N} Learning: Bootstrapping Binary Classifiers by Structural Constraints},
	booktitle = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
	year = {2010},
	pages = {49--56}
}

@article{tldjournal,
abstract = {This paper investigates long-term tracking of unknown objects in a video stream. The object is defined by its location and extent in a single frame. In every frame that follows, the task is to determine the object's location and extent or indicate that the object is not present. We propose a novel tracking framework (TLD) that explicitly decomposes the long-term tracking task into tracking, learning and detection. The tracker follows the object from frame to frame. The detector localizes all appearances that have been observed so far and corrects the tracker if necessary. The learning estimates detector's errors and updates it to avoid these errors in the future. We study how to identify detector's errors and learn from them. We develop a novel learning method (P-N learning) which estimates the errors by a pair of "experts'': (i) P-expert estimates missed detections, and (ii) N-expert estimates false alarms. The learning process is modeled as a discrete dynamical system and the conditions under which the learning guarantees improvement are found. We describe our real-time implementation of the TLD framework and the P-N learning. We carry out an extensive quantitative evaluation which shows a significant improvement over state-of-the-art approaches.},
author = {Kalal, Zdenek and Mikolajczyk, Krystian and Matas, Jiri},
doi = {10.1109/TPAMI.2011.239},
isbn = {2011030153},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Long-term tracking,bootstrapping,learning from video,real time,semi-supervised learning},
number = {7},
pages = {1409--1422},
pmid = {22156098},
title = {{Tracking-Learning-Detection}},
volume = {34},
year = {2012}
}



@inproceedings{struck,
	author = {Sam Hare and Amir Saffari and Philip H. S. Torr},
	title = {Struck: Structured Output Tracking with Kernels},
	booktitle = {ICCV},
	year = {2011},
	pages = {263-270}
}

@inproceedings{csk,
	author = {Jo{\~a}o F. Henriques and Rui Caseiro and Pedro Martins and Jorge Batista},
	title = {Exploiting the Circulant Structure of Tracking-by-detection with Kernels},
	booktitle = {ECCV},
	year = {2012},
	pages = {702-715}
}

@inproceedings{mosse,
	author = {David S. Bolme and J. Ross Beveridge and Bruce A. Draper and Yui Man Lui},
	title = {Visual Object Tracking using Adaptive Correlation Filters},
	booktitle = {CVPR},
	year = {2010},
	pages = {2544-2550}
}

@inproceedings{rcnn,
	Author    = {Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
	Title     = {Rich feature hierarchies for accurate object detection and semantic segmentation},
	Booktitle = {CVPR},
	Year      = {2014},
	pages = {580-587}
}

@inproceedings{spp,
	Author    = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	Title     = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
	Booktitle = {ECCV},
	Year      = {2014},
	pages = {346-361}
}

@misc{PMT, 
	author = {Piotr Doll\'ar}, 
	title = {{P}iotr's {C}omputer {V}ision {M}atlab {T}oolbox ({PMT})}, 
	howpublished = {\small\url{http://vision.ucsd.edu/~pdollar/toolbox/doc/index.html}} 
}

@online{bingImpl, 
	author = {Tianfei Zhou}, 
	title = {BING Objectness proposal estimator Matlab (mex-c) wrapper}, 
	url = {https://github.com/tfzhou/BINGObjectness},
	year = {2015}
}

@Article{voc2007, 
	author = "Everingham, M. and Eslami, S. M. A. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.", 
	title = "The Pascal Visual Object Classes Challenge: {a retrospective}", 
	journal = "IJCV", 
	volume = "111", 
	year = "2015", 
	number = "1",  
	pages = "98-136", 
} 


@inproceedings{edgeboxes,
	author    = {C. Lawrence Zitnick and Piotr Doll\'ar},
	title     = {{Edge Boxes}: locating Object Proposals from Edges},
	booktitle = {ECCV},
	year      = {2014},
	pages = {391-405}
}

@article{selectivesearch,
	author = {Jasper R. R. Uijlings and Koen E. A. van de Sande and Theo Gevers and Arnold W. M. Smeulders},
	title = {Selective Search for Object Recognition},
	journal = {IJCV},
	volume = 104, 
	number = 2, 
	pages = {154-171}, 
	year = 2013
}

@inproceedings {mcg,
	title = {Multiscale Combinatorial Grouping},
	booktitle = {CVPR},
	year = {2014},
	author = {Pablo Arbelaez and Pont-Tuset, J. and Barron, Jon and Marqu{\'e}s, F. and Jitendra Malik},
	pages = {328-335}
}


@article{cpmc,
	author = {Jo{\~a}o Carreira and Cristian Sminchisescu },
	title = {{CPMC}: Automatic Object Segmentation Using Constrained Parametric Min-Cuts},
	journal = {TPAMI},
	volume = 34, 
	number = 7, 
	pages = {1312-1328}, 
	year = 2012
}


@inproceedings{geodesic,
	title={Geodesic Object Proposals},
	author={Philipp Kr{\"a}henb{\"u}hl and Vladlen Koltun},
	booktitle={ECCV},
	year={2014},
	pages = {725-739}
}

@article{objectness,
	author = {Alexe, Bogdan and Deselaers, Thomas and Ferrari, Vittorio},
	title = {Measuring the Objectness of Image Windows},
	journal = {TPAMI},
	volume = 34, 
	number = 11, 
	pages = {2189-2202}, 
	year = 2012
}

@inproceedings{bing,
	title={{BING}: Binarized Normed Gradients for Objectness Estimation at 300fps},
	author={Ming-Ming Cheng and Ziming Zhang and Wen-Yan Lin and Philip H. S. Torr},
	booktitle={CVPR},
	year={2014},
	pages = {3286-3293}
}

@article{dpsurvey,
	author = {Jan Hosang and Rodrigo Benenson and Piotr Doll\'ar and Bernt Schiele},
	title = {What makes for effective detection proposals?},
	journal = {TPAMI},
	year = 2015,
	doi = {10.1109/TPAMI.2015.2465908}
}


@inproceedings{dpcompare,
	author = {J. Hosang and R. Benenson and B. Schiele},
	title = {How good are detection proposals, really?},
	booktitle = {BMVC},
	year = {2014}
}

@inproceedings{structurededge,
	author = {Piotr Doll\'ar and C. Lawrence Zitnick},
	title = {Structured Forests for Fast Edge Detection},
	booktitle = {ICCV},
	year = {2013},
	pages = {1841-1848}
}

@inproceedings{decitree,
	author    = {Wang, Aiping and Wan, Guowei and Cheng, Zhiquan and Li, Sikun},
	title     = {An incremental extremely random forest classifier for online learning and tracking},
	booktitle = {ICIP},
	year      = {2009},
	pages = {1449-1452}
}


@article{multicue,
	author = {Wang, Aiping and Cheng, Zhiquan and Martin, Ralph R. and Li, Sikun},
	title = {Multiple-cue-based visual object contour tracking with incremental learning},
	journal = {LNCS},
	volume = 7544, 
	pages = {225-243}, 
	year = 2013
}

@inproceedings{jots,
	author = {Longyin Wen and Dawei Du and Zhen Lei and Stan Z. Li and Ming-Hsuan Yang},
	title = {{JOTS}: Joint Online Tracking and Segmentation},
	booktitle = {CVPR},
	year      = {2015},
	pages = {2226-2234}
}

@inproceedings{pfseg,
	author = {Vasileios Belagiannis and Falk Schubert and Nassir Navab and Slobodan Ilic},
	title = {Segmentation based particle filtering for real-time 2d object tracking},
	booktitle = {ECCV},
	year      = {2012},
	pages = {842-855}
}

@inproceedings{pixeltrack,
	author = {Stefan Duffner and Christophe Garcia},
	title = {{PixelTrack}: A Fast Adaptive Algorithm for Tracking Non-Rigid Objects},
	booktitle = {ICCV},
	year      = {2013},
	pages = {2480-2487}
}

@inproceedings{houghtrack,
	author = {Martin Godec and Peter M. Roth and Horst Bischof},
	title = {Hough-based tracking of non-rigid objects},
	booktitle = {ICCV},
	year      = {2011},
	pages = {81-88}
}

@inproceedings{vot2013,
	author = {Matej Kristan and Roman Pflugfelder and Ales Leonardis and {\em et al}},
	title = {The Visual Object Tracking {VOT2013} challenge results},
	booktitle = {IEEE International Conference on Computer Vision Workshop},
	year      = {2013},
	pages = {98-111}
}

@inproceedings{vot2015,
abstract = {The Visual Object Tracking challenge 2015, VOT2015, aims at comparing short-term single-object visual trackers that do not apply pre-learned models of object appearance. Results of 62 trackers are presented. The number of tested trackers makes VOT 2015 the largest benchmark on short-term tracking to date. For each participating tracker, a short description is provided in the appendix. Features of the VOT2015 challenge that go beyond its VOT2014 pre-decessor are: (i) a new VOT2015 dataset twice as large as in VOT2014 with full annotation of targets by rotated bounding boxes and per-frame attribute, (ii) extensions of the VOT2014 evaluation methodology by introduction of a new performance measure. The dataset, the evaluation kit as well as the results are publicly available at the challenge website 1 .},
author = {Kristan, Matej and Pflugfelder, Roman and Leonardis, Ale{\v{s}} and Matas, Jiri and {\v{C}}ehovin, Luka and Nebehay, Georg and Voj{\'{i}}$\backslash$vr, Tom{\'{a}}{\v{s}} and Fernandez, Gustavo and Others},
booktitle = {IEEE International Conference on Computer Vision Workshop},
doi = {10.1109/ICCVW.2015.79},
isbn = {9780769557205},
issn = {16113349},
pages = {1--27},
title = {{The Visual Object Tracking {VOT2015} challenge results}},
year = {2015}
}

@inproceedings{mdnet,
abstract = {We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network (CNN). Our algorithm pretrains a CNN using a large set of videos with tracking ground-truths to obtain a generic target representation. Our network is composed of shared layers and multiple branches of domain-specific layers, where domains correspond to individual training sequences and each branch is responsible for binary classification to identify target in each domain. We train each domain in the network iteratively to obtain generic target representations in the shared layers. When tracking a target in a new sequence, we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer, which is updated online. Online tracking is performed by evaluating the candidate windows randomly sampled around the previous target state. The proposed algorithm illustrates outstanding performance in existing tracking benchmarks.},
archivePrefix = {arXiv},
arxivId = {1510.07945},
author = {Nam, Hyeonseob and Han, Bohyung},
booktitle = {arXiv preprint arXiv:1510.07945},
doi = {10.1109/CVPR.2016.465},
eprint = {1510.07945},
isbn = {9781467388511},
issn = {10636919},
pages = {4293--4302},
title = {{Learning Multi-Domain Convolutional Neural Networks for Visual Tracking}},
url = {http://arxiv.org/abs/1510.07945},
year = {2015}
}


@inproceedings{dgt,
	author = {Cai, Z. and Wen, L. and Yang, J. and Lei, Z. and Li, S.Z.},
	title = {Structured visual tracking with dynamic graph},
	booktitle = {ACCV},
	year      = {2012},
	pages = {86-97}
}

@inproceedings{proposalSelect,
	author = {Yang Hua and Karteek Alahari and Cordelia Schmid},
	title = {Online Object Tracking with Proposal Selection},
	booktitle = {ICCV},
	year      = {2015},
	pages = {3092-3100}
}

@article{adobing,
	author = {Pengpeng Liang and Yu Pang and Chunyuan Liao and Xue Mei and Haibin Ling},
	title = {Adaptive Objectness for Object Tracking},
	journal = {IEEE Signal Processing Letters},
	volume = 23, 
	number = 7, 
	pages = {949-953}, 
	year = 2016
}

@inproceedings{moca,
	author = {Guibo	Zhu and Jinqiao Wang and Yi Wu and Xiaoyu Zhang and Hanqing Lu},
	title = {{MC-HOG} Correlation Tracking with Saliency Proposal},
	booktitle = {AAAI},
	year      = {2016},
	pages = {3690-3696}
}

@inproceedings{ebt,
	author = {Gao Zhu and Fatih Porikli and Hongdong Li},
	title = {Beyond Local Search: Tracking Objects Everywhere with Instance-Specific Proposals},
	booktitle = {CVPR},
	year      = {2016},
	pages = {943-951}
}

@inproceedings{rpnt,
	author = {Gao Zhu and Fatih Porikli and Hongdong Li},
	title = {Robust Visual Tracking with Deep Convolutional Neural Network based Object Proposals on {PETS}},
	booktitle = {CVPR workshop},
	year      = {2016},
	pages = {26-33}
}

@inproceedings{rpn,
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	title = {Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	booktitle = {NIPS},
	year      = {2015},
	pages = {91-99}
}



@misc{Authors06,
 author = {Authors},
 title = {The frobnicatable foo filter},
 note = {ECCV06 submission ID 324. Supplied as additional material {\tt eccv06.pdf}},
 year = 2006
}

@misc{Authors06b,
 author = {Authors},
 title = {Frobnication tutorial},
 note = {Supplied as additional material {\tt tr.pdf}},
 year = 2006
}

@article{Alpher02,
author = {A. Alpher},
title = {Frobnication},
journal = {Journal of Foo},
volume = 12, 
number = 1, 
pages = {234--778}, 
year = 2002
}

@article{Alpher03,
author = {A. Alpher and J.~P.~N. Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13, 
number = 1, 
pages = {234--778}, 
year = 2003
}

@article{Alpher04,
author = {A. Alpher and J.~P.~N. Fotheringham-Smythe and G. Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14, 
number = 1, 
pages = {234--778}, 
year = 2004
}

@article{Mermin89,
author = {N. David Mermin},
title = {What's wrong with these equations?},
journal = {Physics Today},
year = 1989,
month = oct,
note = {\small\url{http://www.cvpr.org/doc/mermin.pdf}}
}

@Book{Hartley00,
  author       = "Hartley, R.~I. and Zisserman, A.",
  title        = "Multiple View Geometry in Computer Vision",
  year         = "2000",
  publisher    = "Cambridge University Press, ISBN: 0521623049",
}

@InProceedings{Claus05,
  author       = "Claus, D. and Fitzgibbon, A.~W.",
  title        = "A Rational Function Lens Distortion Model for General
                 Cameras",
  booktitle    = "Proc. CVPR",
  year         = "2005",
  pages        = "213--219",
}

@InProceedings{Zhang96,
  author =	 "Z. Zhang",
  title =	 "On the Epipolar Geometry Between Two Images With
                  Lens Distortion",
  booktitle =	 "Proc. ICPR",
  year =	 "1996",
  pages =	 "407--411",
}

@InProceedings{Fitzgibbon01,
  author       = "Fitzgibbon, A.~W.",
  title        = "Simultaneous Linear Estimation of Multiple View
                 Geometry and Lens Distortion",
  booktitle    = "Proc. CVPR",
  year         = "2001"
}

@article{Brown71,
author = {D.~C. Brown},
title = {Close-range camera calibration},
journal = {Photogrammetric Eng.},
volume = 37,
number = 8,
pages = {855--866}, 
year = 1971
}

@article{Devernay01,
author = {F. Devernay and O. Faugeras},
title = {Straight lines have to be straight},
journal = {MVA},
volume = 13,
pages = {14--24}, 
year = 2001
}

@article{Swaminathan00,
author = {R. Swaminathan and S. Nayar},
title = {Nonmetric calibration of wide-angle lenses and polycameras},
journal = {IEEE T-PAMI},
volume = 22,
number = 10,
pages = {1172--1178}, 
year = 2000
}

@InProceedings{Tsai86,
  author =	 "Tsai, Y.~R.",
  title =	 "An Efficient and Accurate Camera Calibration
                  Technique for {3D} Machine Vision",
  booktitle =	 "Proc. CVPR",
  year =	 "1986",
}


@inproceedings{jetson,
author = {Stone, John E. and Hallock, Michael J. and Phillips, James C. and Peterson, Joseph R. and Luthey-Schulten, Zaida and Schulten, Klaus},
booktitle = {IEEE 28th International Parallel and Distributed Processing Symposium Workshops},
doi = {10.1109/IPDPSW.2016.130},
isbn = {9781509021406},
keywords = {Energy efficiency,GPU computing,Heterogeneous architectures,High-performance computing,Mobile computing,Molecular modeling},
pages = {89--100},
title = {{Evaluation of emerging energy-efficient heterogeneous computing platforms for biomolecular and cellular simulation workloads}},
volume = {2016-August},
year = {2016}
}

@misc{top500,
	lab = {TOP500.org},
	author = {TOP500},
	organization = {TOP500.org},
	year =         {2010},
	title =        {{TOP500} lists: November 2010},
	url =          {https://www.top500.org/lists/2010/11/highlights/}
}

@book{oclspec,
author = {Munshi, A},
institution = {Khronos OpenCL Working Group and others},
title = {{The {OpenCL} Specification}},
url = {http://www.khronos.org/opencl},
year = {2011}
}

@inproceedings{fasterrcnn,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Conference on Neural Information Processing Systems},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
isbn = {0162-8828 VO - PP},
issn = {01689002},
pages = {1--10},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}

@inproceedings{fastrcnn,
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
isbn = {9781467383912},
issn = {15505499},
pages = {1440--1448},
pmid = {23739795},
title = {{Fast R-CNN}},
year = {2015}
}

@misc{openmp,
	organization = {OpenMP Architecture Review Board},
	year =         {2015},
	title =        {Open Multi-Processing},
	url =          {http://www.openmp.org/}
}

@book{cudaspec,
author = {{NVIDIA Corporation}},
keywords = {Nvidia},
title = {{CUDA C Programming Guide}},
url = {http://developer.download.nvidia.com/compute/DevZone/docs/html/C/doc/CUDA\_C\_Programming\_Guide.pdf},
year = {2017}
}

@misc{snapdragon,
	organization = {Qualcomm},
	year =         {2017},
	title =        {Snapdragon 835 mobile platform},
	url =          {https://www.qualcomm.com/products/snapdragon/processors/835}
}

@inproceedings{cell,
  title={An {OpenCL} Framework for Heterogeneous Multicores with Local Memory},
  author={J. Lee and J. Kim and S. Seo and S. Kim and J. Park and H. Kim and T. T. Dao and Y. Cho and S. J. Seo and S. H. Lee and others},
  booktitle={Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
  pages={193--204},
  year={2010},
  organization={ACM}
}

@misc{oclppt,
	author = {Tim Mattson and Udeepta Bordoloi},
	year =         {2011},
	title =        {{OpenCL: an Introduction for HPC Programmers}},
	url =          {https://indico.cern.ch/event/138427/sessions/11397/attachments/116552/165428/OpenCL-intro-ISC11.pdf}
}

@inproceedings{lkoptflow,
author = {Lucas, B D and Kanade, T},
booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence},
keywords = {Image Registration},
pages = {674--679},
title = {{An Iterative Image Registration Technique with an Application to Stereo Vision}},
volume = {2},
year = {1981}
}

@inproceedings{lkoptflow2,
author = {Shi, Jianbo and Tomasi, Carlo},
booktitle = {Proceedings of  the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {593--600},
title = {{Good Features to Track}},
year = {1994}
}

@article{ncc,
author = {Lewis, J},
journal = {Vision Interface},
pages = {120--123},
title = {{Fast Normalized Cross-Correlation}},
volume = {10},
year = {1995}
}

@inproceedings{fern,
abstract = {While feature point recognition is a key component of modern approaches to object detection, existing approaches require computationally expensive patch preprocessing to handle perspective distortion. In this paper, we show that formulating the problem in a Naive Bayesian classification framework makes such preprocessing unnecessary and pro-duces an algorithm that is simple, efficient, and robust. Fur-thermore, it scales well to handle large number of classes. To recognize the patches surrounding keypoints, our classifier uses hundreds of simple binary features and mod-els class posterior probabilities. We make the problem com-putationally tractable by assuming independence between arbitrary sets of features. Even though this is not strictly true, we demonstrate that our classifier nevertheless per-forms remarkably well on image datasets containing very significant perspective changes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1407.5736v1},
author = {{\"{O}}zuysal, Mustafa and Fua, Pascal and Lepetit, Vincent},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383123},
eprint = {arXiv:1407.5736v1},
isbn = {1424411807},
issn = {10636919},
pmid = {21736739},
title = {{Fast Keypoint Recognition in Ten Lines of Code}},
year = {2007}
}

@misc{opentld,
	author = {Zdenek Kalal},
	year =         {2011},
	title =        {{OpenTLD}},
	url =          {https://github.com/zk00006/OpenTLD}
}

@article{htld,
abstract = {{\textcopyright} 2015 Springer-Verlag Berlin HeidelbergThe recently proposed tracking–learning–detection (TLD) method has become a popular visual tracking algorithm as it was shown to provide promising long-term tracking results. On the other hand, the high computational cost of the algorithm prevents it being used at higher resolutions and frame rates. In this paper, we describe the design and implementation of a heterogeneous CPU–GPU TLD (H-TLD) solution using OpenMP and CUDA. Leveraging the advantages of the heterogeneous architecture, serial parts are run asynchronously on the CPU while the most computationally costly parts are parallelized and run on the GPU. Design of the solution ensures keeping data transfers between CPU and GPU at a minimum and applying stream compaction and overlapping data transfer with computation whenever such transfers are necessary. The workload is balanced for a uniform work distribution across the GPU multiprocessors. Results show that 10.25 times speed-up is achieved at 1920 (Formula presented.) 1080 resolution compared to the baseline TLD. The source code has been made publicly available to download from the following address: http://gpuresearch.ii.metu.edu.tr/codes/.},
author = {Gurcan, Ilker and Temizel, Alptekin},
doi = {10.1007/s11554-015-0538-y},
issn = {18618200},
journal = {Journal of Real-Time Image Processing},
keywords = {CUDA,Heterogeneous CPU???GPU implementations,Object tracking,Real time},
pages = {1--15},
title = {{Heterogeneous CPU-GPU Tracking-Learning-Detection (H-TLD) for Real-Time Object Tracking}},
year = {2015}
}

@misc{opencv,
abstract = {Open Source Computer Vision is a library of programming functions for real time computer vision},
organization = {Intel Corporation, Willow Garage, Itseez},
title = {{OpenCV Library}},
url = {http://opencv.org/},
year = {2016}
}

@misc{intelocl,
organization = {Intel Corporation},
title = {{OpenCL Drivers and Runtimes for Intel Architecture}},
url = {https://software.intel.com/en-us/articles/opencl-drivers},
year = {2016}
}



