%*********************第一章******************
\chapter{绪论}

\section{研究背景}
异构平台应用越来越广泛，服务器和集群系统都偏向采用异构体系结构，各种异构硬件平台可以相应解决不同的应用领域。科学计算领域对计算需求比较大，需要大规模集群系统才能满足计算的需求。而对于近两年比较火的人工智能领域，也属于计算密集型应用，在异构平台的服务器上可以得到满足。本课题选取了心脏组织模拟以及卷积神经网络应用作为科学计算和人工智能领域的研究例子，对异构平台并行优化技术进行研究。

\subsection{异构平台及异构编程}
异构体系结构编程面临诸多挑战。在单块芯片上集成很多简单小核的硬件加速器在高性能计算系统中占有重要地位。目前两个占统治地位的众核加速器是图形处理单元GPU和Intel的Xeon Phi。现在许多的超级计算机都是包含多核CPU和众核加速器的异构集群。

本课题关注的其中一种加速器类型就是Intel的第一代Xeon Phi众核加速器。Xeon Phi的双精度峰值性能是通过16*核数＊频率计算的。比如1.053GHz 60个核的5110P协处理器具有的峰值性能可达16$\times$ 60 $\times$ 1.053=1011GFLOPS，这里的系数16是因为Xeon Phi的寄存器可达512位即8个双精度，并且每个核支持2个硬件线程。需要指出的是这里的峰值性能意味着使用了所有的硬件资源。

Xeon Phi能提供的巨大峰值性能给高效编程带来巨大的挑战。除了高效使用大量的硬件线程，程序员还需要关注如何发挥每个线程SIMD的能力。前者可以通过使用OpenMP编程来达到，SIMD向量化可以通过编译器的自动向量化或者使用AVX-512手动向量化来取得理想的8倍双精度浮点性能提升。

异构系统中另外一个不容忽视的因素是多核CPU的计算性能。目前，单个多核CPU的核数都是比较少的，通常在8到18之间，然而，每个CPU核都是高频的并且可以乱序执行，因此，CPU核比Xeon Phi的核的能力更强也更灵活。对于SIMD向量化不是很容易实现或者性能的瓶颈是存储带宽的非直接计算，一个典型的八核Sandy Bridge CPU可以取得与众核相当的性能。所以对于CPU＋Xeon Phi的异构系统，同时发挥CPU和Xeon Phi的性能将变得很重要。异构计算除了给编程带来挑战，还涉及其它的任务，比如使用针对CPU核的256位的AVX指令集，对CPU核Xeon Phi间的通信进行优化以及两种不同硬件上负载均衡。

而对于GPU平台来说，GPU呈现的复杂体系结构同样给编程带来了挑战。GPU中大量的并行计算单元支持很多硬件线程同时执行，Nvidia公司针对GPU专门开发了CUDA编程模型，根据CUDA编程模型，程序员可以描述每个线程的行为，以及控制CPU和GPU间的通信等功能，这些都为GPU编程带来了一定困难。此外，GPU提供丰富的存储层次满足计算对带宽的需求，如何通过编程来发挥各个存储层次的存储效率也是值得研究的问题。

\subsection{心脏组织模拟}
并行计算在计算心脏学运用很广。计算心脏学对计算能力的需求在不停的增长，因为计算心脏学专家想采用高级的数学模型来模拟心脏并且在时间和空间上都使用更高 的分辨率。由于异构超级计算机的超强计算能力，它们越来越受到欢迎。

计算心脏学中一个很重要的研究课题就是发生在心脏组织和器官级的心率失常。但由于有限的计算能力，大部分之前的研究只是集中在心脏细胞内电压行为以及钙离子处理的模型上，钙离子处理过程是通过对亚细胞中随机钙离子处理\upcite{gaur2011multiscale,nivala2012computational,restrepo2008calsequestrin,williams2011dynamics}离散化实现的。这些钙离子处理和电压行为的模型对于研究心率失常的起因和原理的研究非常有益。心率失常主要表现形式有后除级\upcite{GPUcell}和早后除级\upcite{nivala2012calcium,nivala2015t}。

但是将细胞级的研究推广到对组织或者器官级的心率失常是不够的，这种挑战从模型到计算。人体的心脏大概有2 $\times$ $10^9$个细胞\upcite{adler1974cell}，每个心脏细胞内又有$10^6$个RyRs(ryanodine receptors)，这些RyR分布在大概$10^4$个钙离子释放单元中，每个dyad有15个L-type的通道，这些通道对电压和本地钙离子浓度做出反应\upcite{cheng1993calcium}。两个8核的Sandy Bridge CPU需要1秒钟去模拟3000个心脏细胞1个时间步。而由于一个现实的组织模拟涉及到$10^7$ $\sim$ $10^8$个细胞$10^4$ $\sim$ $10^5$个时间步，所以心脏模拟对超级计算的需求是很显然的。

\subsection{卷积神经网络应用与加速}
对于卷积神经网络应用来说，用户希望能够尽快得到计算结果。而现有的神经网络却越来越复杂，如何在现有的硬件上实现一个低延迟的卷积神经网络变得尤为重要。卷积神经网络中的计算主要集中在卷积层，传统的卷积计算方法计算量大，特别是对于3D卷积计算来说，计算量和存储都会使现有的硬件资源无法满足。

为了降低卷积神经网络前向计算过程的延迟，采用异构GPU加速器进行加速，更重要的是改进算法，从卷积的算法实现上降低总的计算量，这对于加速卷积神经网络的计算过程具有重要的作用。本课题主要是针对3D卷积计算提出了新的3D卷积实现算法，该算法能够从理论上大幅度降低浮点计算量，并将新的3D卷积算法高效地映射到异构GPU体系结构中。

加速卷积神经网络前向计算过程还有一种方法就是采用多GPU设备并行的加速方案。而目前针对神经网络的多GPU加速方案主要采用的是数据并行方法，该方法可以有效提高神经网络处理的吞吐率，但无法降低神经网络的延迟。多GPU环境下，采用模型并行会带来通信问题，即在每层卷积层计算结束后，需要进行通信，通信开销与设备的数目成正比。因此设计一个高效的通信模式对性能影响也至关重要。

\section{研究意义}
以心脏组织模拟这个科学计算为背景，采用多种并行优化手段将心脏组织模拟高效地 映射到超级计算机上比如天河 2 号。对于研究生物信息学领域的专家来说，可以验证他们 提出的心脏模型是否可靠，同时很多研究可以不用在具体的人身上做实验和观察，可以直 接通过改变参数做各种实验,这将极大地加速这方面的研究，所以，这对于心脏疾病的研 究具有重要的意义。

将真实的科学计算映射到天河 2 号异构计算平台，需要面临众多高性能计算的难题。 首先是需要提高单节点的性能，天河 2 号单节点内包括多核的 host CPU 以及 3 个 MIC 加速 卡，本课题采用 SIMD 向量化、OpenMP 以及 COI/SCIF 编程技术可以很好地解决天河 2 号 单节点各个计算单元的协同计算，以及很好地充分利用了各级并行。而在节点间采用 MPI 并行技术并且对 MPI 通信采取一定的优化措施，保证大规模异构节点的扩展性。所以本研 究的各项技术可以解决在天河 2 号上面向特定应用的编程问题，将使得天河 2 号在解决真实的科学计算问题上再向前迈一步。

基于GPU异构平台，采用新的卷积算法以及实现多GPU下的模型并行方法，能够有效加速卷积神经网络前向计算过程。同时这可以很好地提取卷积神经网络计算特征，并发现现有GPU体系结构在神经网络计算领域中存在的优势和不足，这对研究新型或者专用加速器具有重要的指导意义。

\section{研究现状}

\subsection{针对异构平台的并行优化技术研究}
基于加速器的异构系统是指使用不同功能或性能的计算设备，通过一定结构互联而构 成的计算机系统，其中一种计算设备的计算能力强于传统的通用处理器,被称为加速器， 用于实现对应用程序需要密集计算部分的计算加速。典型的加速器包括数字信号处理器 (DSP)、面向通用计算的图像处理器(GPU)、众核协处理器(MIC)以及硬件加速器(FPGA) 等。

异构体系结构应用非常广泛了，从各种嵌入式平台到高性能超级计算机，都采用了异构体系结构。QUALCOMM公司开发的Snapdragon 810嵌入式处理器\upcite{qualcomm}的峰值性能达到388Gflops，已经应用到手机和平板等各种嵌入式设备上。ARM 公司开发的 Mali-T760 \upcite{mali}嵌入式处理器峰值性能326GFlops。Imagination公司的PowerVR GT7900\upcite{powerVR}处理器峰值性能达552GFlops。NVIDIA 推出的 Jetson TX1 开发板\upcite{jetson}性能已经能达到 1TFlops。而在高性能计算领域，异构体系结构更是不可或缺。当时的天河-1号\upcite{tianhe1}超级计算机采用的是CPU+GPU的异构体系结构，而天河-2号\upcite{tianhe2}采用的是CPU+MIC的结构。采用异构系统的超级计算机还有Titan\upcite{titan}、Piz Daint\upcite{piz}以及Stampede\upcite{stampede}等。

对异构系统的开发需要采用异构编程模型来实现。目前针对 NVIDIA GPU 主要采用 CUDA\upcite{luebke2008cuda}编程，也可以采用 OpenCL\upcite{stone2010opencl}编程。文献\upcite{karimi2010a}对 CUDA 和 OpenCL 两种编程模型在GPU上的性能做了对比。由于 OpenCL 是面向各种平台的框架，因此OpenCL的性能移植性\upcite{wen2015improving}并不是很好，针对不同的平台需要做专门的优化。针对 FPGA 异构平台的开发，目前已经开始支持OpenCL编程，这意味着FPGA开发的门槛将降低，但从目前看OpenCL在FPGA上的性能并不是很好。但只要随着FPGA对OpenCL的支持更加的完善，FPGA将成为一股新的热潮。Intel的众核协处理器 MIC\upcite{more2013intel}的编程模式可以有多种，比如有Native模式，这种方法可以直接将通用的C程序直接移植都 MIC 上。还支持 offload 模式\upcite{Newburn2013}，Offload 模式有两种，一种是采用类似OpenMP\upcite{dagum1998openmp}的方法，在程序中添加一些编译指导语句，将核心计算部分加载到 MIC 中执行，另一种是采用 COI/SCIF\upcite{potluri2013mvapich-prism}编程，这是Intel提供的控制MIC的底层接口，编程将更加复杂，但效率会更高。

各种异构平台在不同应用中都发挥着重要作用。CPU+GPU的异构平台在QR矩阵分解\upcite{agullo2011qr}中取得了很好的效果，充分利用了CPU和GPU两种硬件资源。基于CPU+GPU的异构平台，Jun Chai\upcite{chai2013resource-efficient}充分利用计算资源解决了贝叶斯推断问题，在生物信息比对中，Bo Chen等人\upcite{chen2010a}将Smith-Waterman算法在GPU异构平台上进行了并行化实现，而Haidong Lan等人\upcite{lan2016parallel}在Xeon Phi的集群系统中实现了生物信息比对中的算法。Siddharth\upcite{choudhary2010practical}在异构GPU上实现了实时的3D重构。Joshua Peraza等人\upcite{peraza2013understanding}在Intel Xeon Phi上对stencil计算的性能进行了评估，而Jian Tao等人\upcite{tao2012using}采用GPU对stencil计算进行加速用于解决大规模的科学应用。异构平台在其它应用领域\upcite{chen2012accelerating, lu2015mrphi, conti2012gpu, junior2012a}也发挥着重要作用。

涌现了大量的针对异构计算的优化技术的研究。异构计算中需要解决多个重要问题，诸如异构平台中的CPU和加速器间的任务调度问题，文献\upcite{acosta2013dynamic}采用了一种动态的负载均衡策略，Ana Balevic等人针对异构平台中的数据流执行提出了流缓冲机制\upcite{streambuffer}，Michela等人解决了异构平台下分布式存储中的数据调度问题\upcite{becchi2010data-aware}，Alecio等人则提出了一种有效的动态调度运行时以及可调系统\upcite{binotto2011an}，针对Intel Xeon Phi加速器的SIMD技术\upcite{tian2015effective}可以有效利用计算资源。

\subsection{心脏组织数值模拟的研究现状}
在组织级（tissue-level）来模拟心脏生物电行为对计算能力具有极大需求\upcite{niederer2011verification}。同时，复杂的心脏模型包含了上百个状态变量，因此需要强大和计算密集的解法来求解常微分方程（ODE）\upcite{hartman2002ordinary}。如果需要快速模拟，如几小时，来模拟整个心脏循环，就必须要使用非常大的并行计算机。文献\upcite{niederer2011simulating}中，通过在 16384个CPU 核上扩展并行而完成了对于人类心脏生物电生理的近实时的单域模拟（1 分钟之内）。随着使用能发挥大量并行的计算能力的，相对新的通用 GPU 的发展趋势，许多计算科学的领域以已经开始采用这种硬件技术。计算心脏学也无例外地使用了GPU 计算。文献\upcite{rocha2011accelerating}中对于 2 维度心脏模拟，对心脏细胞膜的单域方程和 ODE 在GPU 上进行了求解，获得相对于 4 核 CPU 的 20 倍的加速。Bartocci 等人\upcite{bartocci2011toward}展示了使用单个 GPU，通过仔细的和面向模型的优化，包括真实和细节化的心脏细胞模型的 2 维 /3 维模拟能够以接近实时执行。之前的已有研究单域心脏模拟使用少量的 GPU。在文献\upcite{sato2009acceleration}中，使用 4 个 GPU 的心脏单域模拟 3 维代码展示了相比 32个 CPU 核的 1.6 倍加速。Vigmond 等人\upcite{vigmond2009near-real-time}展示了对于求解心脏单域模型的一套非线性的 ODE，相比 4 个 CPU 核，在 4 个 GPU 上能够加速 9–17 倍。在文献\upcite{nimmagadda2012cardiac}中，心脏双域模拟被移植到一个 4 GPU 的平台，对于基本的显式的数值方法进行了有效的面向体系结构的优化，并通过细粒度的并行策略，使得对于 3 维模拟相比单个 CPU 核获得巨大的改进（加速比 2460）。在文献\upcite{neic2012accelerating}中，通过使用当前主流的兔子心室模型，展示了心脏双域模拟运行在 6–20 个 GPU 上能够获得性能好处和扩展性。对于并行化一个 2 维单域模型，最近的工作就是使用混合CPU-GPU的方法，展示了当运行于 8 节点的阵列时，相比只有 CPU 的实现（使用 64 个CPU 核），混合实现（使用 64 个 CPU 核以及 16 个 GPU）获得了近似 7 倍的加速\upcite{barros2012simulations}。

 纳米精度的亚细胞级钙动力数值模拟可以作为一个重要的工具，用于探索很多心脏疾病的生理原因。已经广泛建立了很多计算模型，通过计算模拟来更好地理解钙信号以及钙波动产生的复杂动力学过程\upcite{izu2013ca2+}。然而，基本的挑战来自于其中包括极其不同的长度量级：t-细管和 SR 之间的裂缝范围在 10 nm以内\upcite{franziniarmstrong1999shape, hayashi2009three-dimensional}，单个 RyR 的通道口是 1 nm大小\upcite{serysheva2005structure}，而细胞规模从 10 到 100 µm 不等。当前建立的亚细胞钙波动生成模型为了研究整个细胞，而对钙动力学折中了部分细节\upcite{nivala2012computational}，且只模拟到 0.2um 的网格精度。他们将一个 CRU 的动力学与单个的随机变量相结合，来代替对单独 RyR 的随机调整。即无论何时钙离子释放发生，他们都将其作为单点的来源。当前的模拟研究主要障碍之一就是巨大的计算需求。例如，为了在单个肌纤维节内求解纳米级规模的 CRUs，会占用一个体积为 10 × 10 × 2µm3 的 3 维空间，需要 2 × 1011 个体积单元, 每个单元的体积为 1 nm3。体积单元的数量总是伴随这巨大数量的时间步需求。为了模拟一个肌纤维 1 ms，可能需要总的浮点操作数在$10^19$的量级\upcite{chai2015towards}。因此使得亚细胞钙动力学在纳米精度的模拟极具挑战性。已有的研究或者去掉细节，或者只模拟非常小的空间域，欠缺实际物理意义。而真实的研究最理想的是包括多个肌纤维节，并模拟超过数百毫秒。由此可见，在可预见的未来，在纳米精度下亚细胞级钙动力学模拟都将是一个极具挑战性的课题。

\subsection{卷积神经网络快速实现的相关研究}

随着卷积神经网络变得越来越复杂，计算量越来越大，异构平台成为卷积神经网络实现的有效平台。目前主流的异构平台主要是GPU、FPGA以及TPU，GPU平台主要优点是编程简单，加速效果明显，FPGA平台的峰值性能一般没有GPU高，但效率高，能耗低，在某些场景下具有优势，TPU是Google公司专为神经网络设计的加速器，在神经网络的计算过程中，可以达到很高的峰值性能。

在GPU异构平台上针对卷积神经网络进行加速的工作已经很多了。文献\upcite{krizhevsky2014one}对卷积神经网络在GPU异构平台上进行了并行化，文献\upcite{scherer2010accelerating}在GPU异构平台上对卷积神经网络的训练过程进行加速，最后取得了很好的加速效果，文献\upcite{strigl2010performance}将卷积神经网络中计算密集的部分分配到GPU中进行加速，在GPU中能够取得很好的性能和可扩展性。文献\upcite{paine2013gpu}在GPU平台中实现了异步随机梯度下降法对神经网络的训练过程进行加速。由于神经网络结构越来越复杂，对神经网络的训练也由单节点转向多节点，文献\upcite{coates2013deep}中的工作是基于GPU集群环境下进行训练的，能够扩展到16个节点中进行训练，而Awan等人提出设计的S-caffe框架\upcite{scaffe}将Caffe\upcite{jia2014caffe}扩展成了多节点版本，并对多节点环境下进行了通信方面的优化，最后在160个GPU上取得了很好的扩展性。

基于FPGA的神经网络加速的相关研究也逐渐变得流行起来。Chen Zhang等人\upcite{zhang2015optimizing}针对现有的FPGA加速器在实现神经网络中空间没有得到充分利用，并且计算吞吐率与带宽存在不能匹配的问题，他们根据roofline模型\upcite{williams2009roofline}提出了一个定量分析的方法论证了在最少使用FPGA资源的情况下完成计算任务。文献\upcite{sankaradas2009a}中的工作主要是将卷积神经网络中中的涉及到的计算都在FPGA中实现了，采用低精度的数据表示来增加有效存储带宽。Yuran Qiao等人\upcite{qiao2016fpga‐accelerated}在FPGA上高效地实现了一个矩阵乘模块，并将该模块用于解决卷积神经网络的卷积层计算，取得了很好的性能，文献\upcite{2017arXiv170103534A}采用OpenCL高级语言在FPGA上进行开发，实现对神经网络的加速。文献\upcite{li2016a, DataflowFPGA}中工作也是基于FPGA对卷积神经网络的加速。

还有加速卷积神经网络的方法是简化神经网络中的计算。Mathieu等人采用傅立叶变换的方法\upcite{mathieu2013fast}实现卷积计算，这种方法在卷积核大小比较大的时候优势明显，因为卷积核大小在一定范围内变化时，比如3～16，傅立叶方法所需的计算量不会随着卷积核大小变化而变化，而对于传统卷积方法来说，卷积核越大意味着卷积所需的计算量越大，因此，在卷积核比较大时，选择傅立叶方法在计算量方面有明显的优势，而傅立叶方法存在的问题就是所需的存储开销大，这成为限制傅立叶方法普遍使用的重要障碍。Lavin等人\upcite{lavin2016fast}将一个称为Winograd的算法应用在2D卷积计算中，并且在GPU平台上高效地实现了Winograd算法。Winograd算法是传统卷积计算方法与傅立叶方法的一种折中，即在保持存储开销不变的条件下降低计算量，最后的实验结果也表明，他们在GPU上的高效实现最后性能超过目前最快的cuDNN库。Winograd算法也非常适合在硬件加速平台上实现，Aydonat等人\upcite{aydonat2017opencl}就将Winograd算法在FPGA平台上采用高级语言进行了实现。另一种加速卷积网络计算的思路是将其中的浮点计算转化为代价更小的计算类型。比如将32位的浮点运算转化为16位浮点或者8位浮点运算，而这基本不会影响结果的精度。有些工作比如\upcite{courbariaux2015binaryconnect}、\upcite{courbariaux2016binarized}甚至将卷积计算的输入和模型参数转化为4位或者2位。而Rastegari等人\upcite{rastegari2016xnor}提出了一个XNOR网络，这个网络的输入和模型参数都是用1位来表示，而它们之间的计算则转化为异或计算，这些方法不但简化了计算而且还节省了存储开销，但存在精度下降的问题。

\section{主要研究内容和创新点}
异构平台在科学计算和人工智能领域逐渐发挥越来越重要的作用。本课题分别从科学计算和人工智能领域各选取一个典型应用即心脏组织模拟和卷积神经网络在异构平台上进行映射。无论是心脏组织模拟应用还是卷积神经网络应用，对计算需求都非常大，其中包含的计算也有各自的特点，根据计算特点，本课题选取了两种典型的异构平台进行实现，一种是基于Intel Xeon Phi加速器的异构平台，一种是基于Nvidia GPU的异构平台。主要涉及的研究内容包括以下几方面：
\begin{compactitem}
\item[1.]心脏组织的3D模拟在大规模多核CPU系统上的映射。本课题提出了一个精细的3D心脏组织模拟模型，这个模型能够模拟人类心脏组织细胞内的电活动和钙离子处理过程。本课题最终是要将心脏组织的3D模拟映射到大规模异构系统中，具体的系统为天河2号系统，对于大规模异构集群系统来说，各节点的主机CPU的性能也不能忽略，因此本课题首先针对大规模集群的多核CPU系统进行优化。

\item[2.]心脏组织模拟的3D模拟在天河2号超级计算系统上的映射。针对Intel Phi加速器采用多种并行技术对心脏组织模拟中的核心计算进行并行优化，根据异构平台体系结构的特点实现了tissue level、cell level以及dyad level的三级并行。

\item[3.]3D卷积神经网络在异构GPU平台上的高性能实现。针对目前3D卷积网络中卷积计算量大的特点，推导出了一种能够有效降低计算量的3D卷积算法，即3D Winograd算法。本课题从理论上证明了该算法能有效降低浮点运算量，并在GPU异构平台上高效地并行实现，最后取得了比当前流行的深度学习库更好的性能。

\item[4.]面向卷积神经网络前向过程的低延迟实现。低延迟作为卷积神经网络前向过程的一个重要指标，为了降低计算时间，本课题在多GPU设备上对卷积神经网络采用模型并行。模型并行需要解决设备间的通信问题，因此，本课题还需要设计一个高效的通信模式，一方面降低通信时间，另一方面尽量隐藏通信时间。

\end{compactitem}

创新点有以下几点：
\begin{compactitem}
\item[1.]提出了心脏组织细胞的精细数学模型并且实现一个基于大规模多核CPU系统的心脏组织的3D模拟器。能够模拟一定规模的心脏组织的一些简单行为。

\item[2.]面向天河2号异构系统，实现了一个大规模心脏组织的3D模拟器。实现了心脏组织模拟的多级并行，很好地将个部分计算映射到异构集群系统中。

\item[3.]面向GPU异构计算平台，设计和实现了一个快速的3D卷积算法。本课题首次推导和实现了该快速算法的3D形式，并从理论上论证了该算法能够有效降低计算量。

\item[4.]面向多GPU设备，采用模型并行完成了卷积神经网络前向计算过程的低延迟实现。设计了一种环形的通信结构，使得通信开销不会随着GPU设备数目的增多而变得复杂，并且实现了通信与计算的重叠执行。

\end{compactitem}

\section{论文结构}
本论文共分为六章，具体结果介绍如下：

第一章为绪论，主要介绍本课题研究的背景和意义，并对本课题研究的两个典型应用\pozhehao 心脏组织模拟和卷积神经网络加速方面以及面向异构平台的并行优化技术方面的相关研究进行了介绍。

第二章首先介绍了心脏组织的数学模型、心脏细胞的数学模型以及对应的数值方法，然后介绍了心脏组织和细胞的数学模型在多核CPU系统上的并行映射实现。

第三章介绍将心脏组织和细胞的数学模型映射到天河2号系统中，主要介绍了映射过程中的具体并行技术。在试验部分介绍了心脏组织模拟在天河2号系统上单节点的性能以及多节点上的扩展性结果。

第四章介绍基于GPU异构平台下的3D卷积神经网络的快速算法实现。介绍了3D卷积神经网络的快速算法原理、该算法的复杂度分析以及面向GPU异构平台的并行实现。

第五章介绍了在多GPU环境下降低卷积神经网络前向计算过程延迟的方法以及一种通信开销不随GPU数目增多而增大的通信模式设计。此外，还介绍了通信与计算重叠技术的实现。





