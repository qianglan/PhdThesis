%*********************第三章******************
\chapter{跟踪器中``目标候选''的作用分析和优化}
\label{chapijcv}

\section{引言}
在``目标候选''（原英语术语为``Detection Proposal''，这里采用意译）方法出现之前，
物体检测领域通常使用``滑动窗口（Sliding Window）''的方式来检测物体\upcite{slidingdetect1, slidingdetect2}。
滑动窗口指的是无视输入图像的实际内容，直接用一个窗口扫描整幅图像（在输入图像中不断滑动），
同时不断利用分类器判断窗口中的图像块是否包含目标物体。
由于物体可能是任意大小的，上述过程需要使用不同尺度和宽高比的窗口重复多次。
据统计，在常用的物体检测数据集\upcite{voc2007, coco}上，
一次单尺度的滑动窗口就会产生$10^4\sim 10^5$个候选窗口。
若引入不同的尺度和宽高比，窗口数目将高达$10^6\sim 10^7$\upcite{dpsurvey, dpsurvey2}。
因为整个物体检测过程所需时间等于一个窗口的分类时间乘以窗口数目，
巨大的候选窗口数量严重地限制了用于检测的分类器的复杂度，
使得物体分类领域的先进方法难以应用于物体检测任务。

为了减少滑动窗口所带来的巨大计算压力，``目标候选''方法应运而生。
所有目标候选方法均基于同一个假设，即任何包含物体的图像区域都具有一种共同的视觉特性，
该特性使得这些区域能够和背景区域区别开来。
因此，一定存在某种方法，能够以一幅图像为输入，然后输出一系列很可能包含着物体的候选区域。
这样的方法即是目标候选方法。
如果目标候选方法的输出能够足够准确地覆盖绝大部分物体，
且输出的候选区域数目远小于滑动窗口产生的（这在物体检测领域被称作``高召回率''），
那么物体检测过程将获得巨大的速度提升。
此外，使用更加复杂而准确的分类器也成为了可能，从而检测精度也将大幅提高。
经过多年的发展，目标候选方法已经具备了上述作用，在物体检测领域占有重要地位。
无论在经典的PASCAL数据集\upcite{voc2007}上，还是在庞大的ImageNet数据集\upcite{imagenet}上，
当前排名靠前的物体检测器均使用了目标候选方法\upcite{rcnn, detector1, detector2, detector3}。
实际的应用证明，除了上述作用以外，目标候选方法还能减少``错误肯定（False Positive）''对分类器的干扰，
从而进一步提升物体检测的准确性。

上一章中，通过将目标候选生成器EdgeBoxes不加修改地嵌入KCF跟踪器中，
获得了引人注目的尺度和宽高比适应力提升。
其中的整体方法是通用的，可用于将任何跟踪器和任何目标候选生成器进行结合。
该方法的本质在于，将目标候选生成器作为跟踪器运动模型的补充。
在理想情况下，即所有物体都被目标候选准确覆盖时，这种补充作用将会非常有效：
对于密集的运动模型，目标候选的灵活性将弥补运动模型固化的采样模式；
而对于随机的运动模型，目标候选对物体的感知力将弥补随机采样可能错过跟踪目标这一最大缺陷。
但是，上述的理想情况是难以满足的，目标候选生成器必然存在误差甚至错误，
而目前还尚未有任何文献讨论过目标候选的质量对于跟踪精度的影响。
此外，上一章仅对EdgeBoxes进行了参数调优，并没有对本质算法进行面向跟踪任务的优化，
因此KCFDP的性能还有着一定的提升空间。
本章将针对上述两个问题进行深入的研究。
一方面，通过将多个目标候选生成器逐一地与上一章的跟踪器进行结合，揭示了目标候选的质量和跟踪精度间有着密切的关系。
另一方面，通过为EdgeBoxes加入``背景抑制''这一优化步骤，以极低的时间开销提升了EdgeBoxes对于跟踪任务的适应性，
从而提高了整体跟踪性能。

本章的内容安排如下：
第2节介绍与本章相关的研究工作，主要是典型的目标候选生成器及它们在视觉跟踪中的应用；
第3节介绍目标候选生成器EdgeBoxes，并对其进行面向跟踪任务的优化；
第4节介绍如何将其它5个目标候选生成器和跟踪器相结合；
第5节进行实验，将分析目标候选质量对于跟踪精度的影响，并评测优化后的EdgeBoxes带来的性能提升；
第6节进行本章小结。



\section{相关研究}
``兴趣点检测（Interest Point Detection）\upcite{interestpoint1, interestpoint2}''和
``显著性检测（Saliency Detection）\upcite{saliency1, saliency2}''
在目标候选方法出现之前就是视觉领域重要的研究课题。
它们和目标候选方法有着相似的动机，但研究背景有所不同。
兴趣点检测和显著性检测主要依据仿生学原理，研究人的视觉系统规律，目的是从图像中找出能够吸引人类注意的突出区域和特别区域。
因此，它们常被用于物体分类、图像检索、图像对比等领域，以大幅减少计算开销。
而目标候选方法专注于物体检测，其本质在于研究物体所在的图像区域（而不是视觉上的显著区域）所具有的视觉特性，
即``物体性（Objectness）''。

目标候选方法在近几年发展迅速，形成了两种主要类型，
即``区域分组（Grouping）''类方法和``窗口评分（Window Scoring）''类方法\upcite{dpsurvey}。

SelectiveSearch\upcite{selectivesearch}是一种典型的区域分组方法，它首先利用边缘提取将图像划分为大量``超像素（Super Pixel）''，
然后使用贪心法将超像素进行合并，得到可能包含物体的各个区域。超像素合并的标准基于人工设计的特征和相似性度量，无需任何训练过程。
SelectiveSearch的应用在当前是最为广泛的，R-CNN\upcite{rcnn}和Fast R-CNN\upcite{fastrcnn}均基于它进行物体检测。
CPMC\upcite{cpmc}首先通过设置不同的``种子点（Seed）''和分割参数来进行多次图像分割（Graph Cut），
然后根据多种中层图像特征，将分割得到的图像分块进行筛选和排序，以得出可能包含物体的区域。
Geodesic\upcite{geodesic}会预先训练一个用于在图像中放置种子点的分类器。
基于该分类器放置的种子点，Geodesic将进行前背景分割，并利用分割结果计算``带符号的测地线距离变换（Signed Geodesic Distance Transformation，SGDT）''。
最后，超像素将根据不同的SGDT等级设置进行合并，得到目标候选区域。

与区域分组类的方法不同，窗口评分类方法通常产生密集的候选窗口，然后对候选窗口进行评分，以计算窗口内包含物体的可能性。
一些窗口评分方法甚至仍采用滑动窗口的方式来产生候选窗口，但是由于此时无需进行实际的物体分类，
其评分函数可以设计得非常高效，足以满足时效性要求。
Objectness\upcite{objectness}通常被认为是最早的目标候选生成器。
它首先进行显著性检测，并在显著区域附近提取大量候选窗口，
然后基于多种线索，利用贝叶斯框架对这些窗口的``物体性''进行评分。
Objectness的评分函数所采用的线索包括颜色、边界、位置、大小、以及非常有效的``超像素跨越''线索。
Bing\upcite{bing}的核心部分是一个以``规范化梯度（Norm of Gradients）''为输入特征的线性分类器。
经过在大型物体检测数据集上的训练，该分类器能够判断图像块内包含物体的概率。
通过对规范化梯度这一特征进行二进制近似，该线性分类器可变得极其高效，仅需几个原子操作即可完成。
因此即使Bing采用滑动窗口的方式提取候选窗口，其效率仍然是目前最高的。

上一章和本章所使用的EdgeBoxes\upcite{edgeboxes}也属于窗口评分类方法。
它首先进行边缘（Edge）提取，然后将边缘组合成边缘组（Edge Group），再考虑边缘组组成的物体边界（Contour）。
EdgeBoxes也采用了滑动窗口的方式，并且在给候选窗口评分时，认为完全包含在窗口内的边界数量就是``物体性''的体现。
上一章已经展示了EdgeBoxes在跟踪器中的巨大作用，
但同时也暴露出它的一些缺陷。
与物体检测任务不同，视觉物体跟踪仅关注被跟踪的目标物体，且对目标候选的准确度要求更高。
但是在跟踪场景下，EdgeBoxes很容易被非目标的物体边界，如属于背景、遮挡物、其它物体等的边界，所干扰，
从而生成不够准确甚至错误的目标候选。

就本文所知，最早将目标候选方法应用于物体跟踪的是上一章对应的论文\upcite{kcfdp}。
之后，目标候选在视觉跟踪过程中的作用得到了广泛的研究和开发\citep{proposalSelect, adobing, ebt, rpnt, moca}。
不同于上一章的KCFDP\upcite{kcfdp}，ADOBING\upcite{adobing}和\cite{proposalSelect}中的方法均没有
将目标候选作为运动模型的补充，而是将目标候选生成器所获取的``物体性''作为额外的跟踪依据，用于增强跟踪器的观察模型。
ADOBING\upcite{adobing}在初始化跟踪器时，
会利用自适应SVM\upcite{adaptivesvm}，对Bing的分类器进行面向被跟踪目标的修正。
修正后的Bing将作为观察模型的一部分，用于识别跟踪目标。
类似于ADOBING，\cite{proposalSelect}将EdgeBoxes的评分策略加入了跟踪器的辨别模型中，以更准确地区分目标物体和背景。
为了解决视觉跟踪中偶发的偏移问题，MOCA\upcite{moca}会周期性地根据``视觉显著性图''提取候选区域并进行辨别。
而在``视觉显著性图''中提取候选区域的过程也可以看作是一种目标候选生成器，
并且它在MOCA中的作用也是弥补原运动模型的不足。
EBT\upcite{ebt}和RPNT\upcite{rpnt}更进一步，它们直接使用修改后的目标候选生成器作为运动模型。
EBT在EdgeBoxes之上加入了一个针对跟踪目标的线性SVM分类器，以将目标候选重新排序，
使得生成的目标候选不仅包含``物体性''，还具有与跟踪目标的相似性。
RPNT使用在物体检测数据集上训练得到的区域候选网络（Region Proposal Network，RPN）\upcite{rpn}作为运动模型，
同时也在前一帧目标位置周围进行随机采样。


\section{跟踪器中目标候选生成器的优化}
通过加入上一章\ref{featureintegrasec}节中的特征整合和鲁邦更新优化，
KCFDP的跟踪器部分获得了显著的精度和鲁棒性提升。
但是作为尺度和宽高比适应力的来源，目标候选生成器在跟踪过程中仍然具有决定性的作用。
例如，如果生成的目标候选质量普遍较差，那么即使具有最高相关滤波响应值的目标候选，也无法用于描述目标物体的位置和大小。
此外，过多错误或者偏差的目标候选还会增加相关滤波器的误判概率，尤其是在背景较为杂乱时。
更严重的是，当错误的目标候选被相关滤波器误判时，该目标候选将被用于更新跟踪器的模型，造成模型的污染，
从而可能导致跟踪偏离目标。
因此，对目标候选生成器进行优化也是十分必要的。

目标候选生成器通常是``类别无关''的，即是说，它们的设计初衷就是生成能够覆盖图像中所有物体的目标候选，
包括那些背景物体和``不起眼''（低显著性）的物体。
但是，当目标候选生成器被用于物体跟踪时，我们期望生成的目标候选能够尽量集中于目标物体。
由于EdgeBoxes将物体边界作为唯一的``物体性''判断依据，它生成的目标候选很可能包含非目标物体边界，例如来自背景物体、
遮挡物体和其它背景纹理的边界。
这样的目标候选是不准确的，将会包括非目标的物体或者偏离目标中心。
为了缓解EdgeBoxes的这一缺陷，本节将对其进行优化。
具体地，本节将在滑动窗口和对窗口评分之前，添加一个额外的``背景抑制''步骤，
使得最终生成的目标候选不仅更加集中于目标物体，还能更准确地包围目标物体。

\subsection{目标候选生成器EdgeBoxes}
EdgeBoxes的思想可以概括为：边缘像素很可能属于物体的边界，而包围着完整物体边界的矩形框则很可能包含物体。
之所以强调包围``完整物体边界''，是因为如果一条物体边界跨越了边界框，那么该边界所属的物体将不处于该边界框内。
基于上述思想，EdgeBoxes将根据``被完全包围在边界框内的物体边界数量''来对候选窗口进行评分。

对于一幅输入图像，EdgeBoxes首先使用结构化边缘提取器\upcite{structurededge}为每一个像素计算出边缘响应值和边缘方向。
然后在垂直于各像素边缘方向的方向上进行非极大值抑制（Non-Maximal Suppression，NMS），
仅保留该方向上具有最大边缘响应的像素。
随后将响应值低于$0.1$的像素滤除，并把剩下的像素称作``边缘像素''。
这些边缘像素的响应值记为$m_p$，方向记为$\theta_p$。

边缘像素（Edge）将被组合成边缘组（Edge Group）。
组合的方法为：从各个像素出发进行贪心搜索，不断地将8个方向上邻接的、且边缘方向差别最小的边缘像素加入到边缘组中。
每一次加入新的边缘像素，都将其边缘方向差别累加，直到积累到大于某阈值（这里设置为$\pi/2$）就停止组合过程。
最后，过短的边缘组还将被融入邻近的较长边缘组中。

从直观角度来讲，如果两个边缘组邻接，且趋向相同，那么它们很可能属于同一条物体边界（Contour）；
反之，如果两者互相间隔，或者相邻但连接后形成大曲率曲线，那么它们很可能分属两条不同物体边界。
EdgeBoxes将两个边缘组属于同一条物体边界的可能性用``亲和度（Affinity）''来度量，其计算公式如下：
\begin{equation}
a(\textbf{s}_i,\textbf{s}_j)=|\cos(\theta_i-\theta_{ij})\cos(\theta_j-\theta_{ij})|^\gamma, \label{edge_affinity}
\end{equation}
其中$a(\textbf{s}_i,\textbf{s}_j)$代表边缘组$\textbf{s}_i$和$\textbf{s}_j$间的亲和度，
$\theta_{i}$和$\theta_{j}$分别是两个边缘组的平均边缘方向。
记两个边缘组的平均位置坐标为$x_i$和$x_j$，则上式中的$\theta_{ij}$就是从$x_i$到$x_j$所形成的直线方向。
显然，如果两个边缘组的平均方向（$\theta_{i}$和$\theta_{j}$）与它们相连后的趋向（$\theta_{ij}$和$\theta_{ji}$）相近，
那么它们间将具有较大亲和度。
上式的$\gamma$用于调整亲和度对于方向的敏感度，这里设置为$2$。
此外，如果两个边缘组间隔$2$个像素以上，则亲和度设置为$0$。
EdgeBoxes将计算出任意两对边缘组间的亲和度，以用于窗口评分。
由于大部分的边缘组是互相间隔的，亲和度计算的实际开销并不大。

完成上述的边缘组提取和亲和度计算后，将进入滑动窗口和对各个窗口评分的过程。
在进行滑动窗口之前，为了提高计算效率，
EdgeBoxes会预先为图像中的每一个边缘组$\textbf{s}_i$计算出组内像素的总响应值，记为$m_i$。
然后为每个边缘组随机提取一个像素$\overline{x}_i$，并记录下来，以快速判断各边缘组是否在某窗口内。
滑动窗口的过程和控制参数已经在\ref{param_setup}节中介绍过了，这里不再赘述，仅详细介绍对一个窗口$b$的评分过程。
为了能够度量``被完全包围在边界框内的物体边界数量''，EdgeBoxes将为每个边缘组$\textbf{s}_i$计算一个$w_b(\textbf{s}_i)\in [0,1]$。
它代表着$\textbf{s}_i$所在的物体边界被完全包含在窗口内的概率。
对于所有跨过窗口$b$的边界的边缘组，记它们组成的集合为$S_b$。
显然如果$\textbf{s}_i\in S_b$，则应有$w_b(\textbf{s}_i)=0$。
类似的，对于任何$\textbf{s}_i$，如果对应的$\overline{x}_i\notin b$，则$w_b(\textbf{s}_i)=0$，因为此时$\textbf{s}_i$只有两种可能：
全部处于窗口$b$之外，或者$\textbf{s}_i\in S_b$。
对于余下的边缘组，即$\overline{x}_i\in b$且$\textbf{s}_i\notin S_b$，按下式计算其$w_b(\textbf{s}_i)$：
\begin{equation}
w_b(\textbf{s}_i)=1-\max_\textbf{T}{\prod_{j=1}^{|\textbf{T}|-1}a(\textbf{t}_j,\textbf{t}_{j+1})}. \label{wbsi}
\end{equation}
上式中，$\textbf{T}$代表一系列边缘组（或者称为一条``路径''），它从某一边缘组$\textbf{t}_1\in S_b$出发，
至$\textbf{t}_{|\textbf{T}|}=\textbf{s}_i$结束，长度为$|\textbf{T}|$。
如果不存在这样的路径，则$w_b(\textbf{s}_i)=1$。
也即是说，上式将找出一条从$\textbf{s}_i$到窗口$b$边界的具有最大亲和度的路径，
然后根据这条路径是一条物体边界的概率（即路径上边缘组亲和度的乘积），对$w_b(\textbf{s}_i)$进行惩罚。
由于大部分边缘组间的亲和度为$0$，因此上述过程也较为高效。

获得$w_b$后，即可为窗口$p$进行评分，评分函数为：
\begin{equation}
	h_b = \frac{\sum_{i}w_b(\textbf{s}_i)m_i}{2(b_w+b_h)^\kappa} - \frac{\sum_{p\in b^{in}}m_p}{2(b_w+b_h)^\kappa}. \label{edgeboxes-full}
\end{equation}
上式和上一节的公式\ref{edgeboxes}虽然形式上不同，但实质是相同的。
这里$i$不再代表像素，而是边缘组的编号。$m_i$为边缘组内像素的总响应值，而$m_p$为像素$p$的边缘响应值。
由于边缘并没有宽度，所以用于惩罚较大窗口的分母是基于周长而不是面积的。
目标候选生成器Objectness\upcite{objectness}已经证明，窗口中心部分的边界信息对于``物体性''几乎没有贡献，
因此最终将减去位于$b^{in}$内的全部像素边缘响应值。

\subsection{使用背景抑制优化EdgeBoxes}
在KCFDP中，如图\ref{process}(b)所示，EdgeBoxes所作用的图像块的中心位于KCF所找出的``初步目标位置''，其尺度大于上一帧的目标大小。
因此完全可以认为，当前帧中的目标物体被包含在该图像块中。
显然，若一条物体边界跨越了该图像块的边界，那么该物体边界必然不属于目标物体。
如本节开头所述，EdgeBoxes极易被非目标物体边界所干扰。
因此如果能够将非目标物体边界全部消除，那么EdgeBoxes生成的目标候选将集中于目标物体，且更加准确。
但上述对非目标物体边界的判断方法并不完美（例如物体大小发生剧烈变化时可能超出图像块范围），
且EdgeBoxes的对物体边界的划分也存在误差（目标物体边界和非目标物体边界间也可能有较高亲和度），


\section{跟踪器中目标候选生成器的适配}
ijcv论文。介绍目的（为了分析目标候选的作用），介绍5个目标候选生成器，介绍如何修改它们以嵌入跟踪器中




\section{实验评测与分析}
\subsection{实验设置}
主要是参数设置

\subsection{跟踪器中目标候选的作用分析}
ijcv论文，对比不同目标候选生成器的准确度和速度

\subsection{跟踪器中目标候选的优化效果评测}
\subsubsection{统计图对比评测}
以下均翻译ijcv论文
\subsubsection{数值化对比评测}
\subsubsection{VOT测试集上的对比}
\subsubsection{参数敏感性分析}

\section{小结}