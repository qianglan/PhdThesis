%*********************第三章******************
\chapter{3D卷积神经网络在异构平台上的高性能实现}
\label{3DWinograd}
\section{引言}
卷积神经网络在图像分类、目标跟踪等很多2D输入的处理任务已经得到了成功的应用。卷积神经网络能够很好地提取特征，所以卷积网络常用于图像分类，比如Alexnet\ref{}、VGG\ref{} 、googlenet\ref{}、resinet\ref{}等卷积神经网络被用来对2D图像进行分类，达到了很好的分类效果。图\ref{}为经典的Alexnet网络，其主要包含大量的卷积层，其中的计算也主要集中在卷积层。

正因为2D卷积网络得到了广泛的应用，所以研究人员开始转向3D卷积神经网络的研究，在\ref{}已经为我们展现了一个名为ObjectNet3D用于3D物体识别的数据库，通过这个数据库的训练，可以达到识别3D的各种姿势的目的，类似于这样的3D数据库还有ShapeNet\ref{}。针对这些3D数据库的3D卷积网络也开始陆续被设计出来，比如Voxnet\ref{}就是一个用于解决3D物体识别的3D卷积神经网络，文章\ref{}提出用3D卷积神经网络解决人类动作识别的方法，此外， 3D卷积神经网络可以处理视频分类的应用\ref{}。

但对于3D卷积神经网络的应用来说，如果仍然采用2D卷积神经网络的方法来处理，就会存在计算量大，存储消耗大的问题。有些方法只能解决其中的一个问题，比如基于FFT变换的方法在某些情况下可以有效降低计算量，但是以消耗存储为代价的。有一种卷积计算的快速算法，称为WMFA(Winograd Minimal Filtering Algorithm)，这种算法目前已经成功运用在2D卷积神经网络中，能够有效降低卷积中的计算量，并且不会增加额外的存储空间。因此，将2D WMFA应用到3D卷积神经网络中是非常值得研究的课题。本章内容主要是介绍3D WMFA算法在3D卷积网络上的应用，需要解决的问题包括，由2D WMFA算法的形式推倒出3D WMFA的形式；从理论上分析3D WMFA算法的复杂性，证明该算法在计算和存储开销上的优势；面向GPU异构平台，将3D WMFA算法高效地映射到GPU上。

本章内容安排如下：第二节介绍了关于卷积神经网络中加速的一些研究现状；第三节介绍快速卷积算法WMFA的3D形式，以及3D WMFA算法的复杂性分析并证明了3D WMFA在理论上可以减少卷积计算量；第四节将重点介绍3D WMFA算法的GPU实现，并且介绍了面向GPU的几种优化技术；第五节将对3D WMFA算法的性能进行评测，并与其它卷积方法的性能进行了对比；最后是对本章内容的总结，随着3D卷积网络应用的推广，对其进行加速变得尤为重要。

\section{相关研究}
对于卷积神经网络的加速，目前主要集中在对2D卷积神经网络的加速研究中，加速的方法主要包括网络结构的改进、算法的改进、深度学习框架中加速库的使用以及新的平台的使用。

Alexnet\ref{}是最早被提出的卷积神经网络，在图像分类应用中，与传统的机器学习分类方法\ref{}相比，分类精度得到大幅度提高，这也开启了对卷积神经网络研究的热潮。为了进一步提高效果，VGG\ref{}网络增加了卷积层的层数（最深多达19层卷积层），这样可以对待分类的图片提取更多的特征，提高分类的精度，虽然所有卷积核大小采用$3\times 3$的卷积核来控制计算量，但由于卷积层数多，计算量仍然很大。Network in Network(NIN)\ref{}是对卷积层的改进，卷积操作是一种线性变换，对特征为线性可分的输入分类效果会很好，而NIN就是解决输入为非线性可分的情况，NIN结构是在卷积层中间引入一个非线性变换操作。GoogleNet\ref{}的设计思路是通过增加网络的深度和宽度来提高分类的精度，但GoogleNet考虑到了计算量的问题，网络结构中引入了很多$1\times 1$的卷积核，该改进可以有效地防止计算量膨胀的问题。 squeezenet\ref{}网络结构在设计上也兼顾了计算量以及模型参数大小两方面，其主要优势就是在保证最后分类精度的情形下减少模型参数。

目前对于比较流行的深度学习框架，比如Caffe\ref{}、Tensorflow\ref{}、Theano\ref{}、Mxnet\ref{}等，它们都采用了专门优化的库实现其中的卷积计算。cblas是面向多核CPU的高效库，cuDNN\ref{}是面向Nvidia GPU的深度学习库。深度学习框架为深度学习开发者提供了一种非常简便的方式来开发深度学习应用，开发者只需要在配置文件中配置使用的平台，就能调用相应的库充分发挥这些底层平台的性能。

现有深度学习框架中调用的库中对卷积的实现大都是将卷积转化为矩阵乘的方法，矩阵分解的方法。。。。还有对卷积计算采用新的等价计算方法，比如FFT的方法\ref{}是将卷积转化为点乘，FFT方法针对卷积核比较大的卷积网络能够显著降低计算量，存在的唯一问题就是存储消耗过大。在卷积算法改进方面的研究还包括本章内容要介绍的应用到3D卷积网络的WMFA方法， Lavin\ref{}在GPU平台高效地实现了WMFA算法，取得了比cuDNN更好的性能。而在文章\ref{}中，作者面向的是FPGA平台，采用OpenCL实现了WMFA算法。无论是GPU平台还是FPGA平台，WMFA算法都表现了很好的并行性。

除了算法方面的改进，也有一些利用集群系统来提高卷积网络的性能。Adam等人开发的COTS HPC系统\ref{}是基于GPU服务器的集群系统，能够对大规模的神经网络进行训练。Google公司使用多年的分布式深度网络框架DistBelief\ref{}使用上千个CPU核心对几十亿的网络参数进行训练。Marthin等人\ref{}对训练的梯度下降法SGD\ref{}提出了一个高效的数据并行算法。H. Wang等人开发的MVAPICH2-GPU\ref{}针对集群中的InfiniBand连接，将CUDA集成在MPICH2中，从而解决了集群系统中的GPU间的通信问题，提高了MPI通信的效率。Mu Li等人\ref{}通过参数服务器的方法提高了在集群上训练的通信效率。

本章的工作主要是借鉴2D卷积网络中算法加速的相关研究，来加速3D卷积神经网络的计算过程。
\section{快速3D卷级算法}
本节将首先介绍3D卷积神经网络中的3D卷积计算的定义，然后通过对1D和2D WMFA算法的介绍，推导出WMFA的3D形式，并将3D WMFA 算法运用到3D卷积层的实现中，最后对3D WMFA算法的计算复杂行进行了分析。

\subsection{3D卷级神经网络定义}
对于2D卷积来说，每个卷积核有长和宽两个维度，卷积核在输入特征图的长和宽两个方向依次滑动进行卷积运算。假定2D卷积层的输入特征图规模为$N\times C\times H\times W$，卷积核规模为$C\times K\times R\times S$，则2D卷积按照其定义可以用公式\ref{eq:2dcnn}表示。
\begin{equation} \label{eq:2dcnn}
Y_{i,k,x,y} = \sum_{c=0}^{C-1}\sum_{r=0}^{R-1}\sum_{s=0}^{S-1}I_{i,c,x+r,y+s}F_{k,r,s,c}
\end{equation} 
3D卷积在2D卷积的基础上增加了深度维度的信息，因此卷积核在做卷积时也需要在深度所在维度进行卷积运算，公式\ref{eq:3dcnn}为3D卷积计算的公式。公式中 $Y_{i,k,x,y,z}$ 代表了第 $k$个通道的卷积计算结果， $I_{i,c,x+r,y+s,z+t}$ 为其中的一个输入特征图， $F_{k,r,s,t,c}$ 是其中的一个卷积核， 公式\ref{eq:3dcnn}是按照3D卷积的定义得到的计算公式，从公式中可以发现3D卷积计算的定义法需要消耗大量的计算，更为详细的复杂行分析将在\ref{Complexity}节中介绍。 

\begin{equation} \label{eq:3dcnn}
Y_{i,k,x,y,z} = \sum_{c=0}^{C-1}\sum_{r=0}^{R-1}\sum_{s=0}^{S-1}\sum_{t=0}^{T-1}I_{i,c,x+r,y+s,z+t}F_{k,r,s,t,c}
\end{equation} 

\subsection{3D WMFA 算法}
本小节将介绍一种快速的3D卷积实现算法，这里命名为3D WMFA算法，WMFA算法由\ref{}提出。WMFA最开始运用在1D和2D卷积计算中。下面将从1D和2D的WMFA形式推导出3D WMFA形式。

在1D的WMFA算法中，一次计算长度为$m$ 的块大小的结果，卷积核大小假定为$r$，则用$F(m,r)$来表示一次$m$个值的计算结果。按照1D卷积计算的定义，需要$2\times 3=6$次矩阵乘运算才能计算出$F(2,3)$，但如果采用1D WMFA算法，则卷积可以通过公式\ref{eq:1dwinograd}实现。
\begin{equation} \label{eq:1dwinograd}
F(2,3) = \begin{bmatrix}
i_0 & i_1 & i_2 \\
i_1 & i_2 & i_3 
\end{bmatrix}
\begin{bmatrix}
f_0 \\
f_1 \\
f_2
\end{bmatrix}
= \begin{bmatrix}
m_1 + m_2 + m_3 \\
m_2 - m_3 - m_4
\end{bmatrix}
\end{equation} 

其中，$m_1$、$m_2$、$m_3$以及$m_4$可以通过公式\ref{mequation}计算得到。

\begin{equation}
\label{mequation}
\begin{split}
m_1 = (i_0-i_2)f_0 	 \quad	m_2 = (i_1+i_2)\frac{f_0+f_1+f_2}{2} \\
m_4 = (i_1-i_3)f_2 	 \quad	m_3 = (i_2-i_1)\frac{f_0-f_1+f_2}{2}
\end{split}
\end{equation}

在1D WMFA中，

\subsection{3D WMFA 算法的复杂性分析}
\label{Complexity}



\section{3D WMFA 算法的实现与优化}

\subsection{3D WMFA 算法的实现}

\subsection{3D WMFA 算法的优化}


\section{实验评测与分析}
\subsection{实验设置}


\subsection{3D WMFA 算法各优化方法的性能}


\subsection{3D WMFA 算法各kernel执行时间分布}


\subsection{3D WMFA 算法与其它卷级算法性能比较}


\section{小结}

