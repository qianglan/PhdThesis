%*********************第四章******************
\chapter{面向卷级神经网络前向过程的低延迟实现}
\section{引言}
随着卷级神经网络的应用越来越广，数据中心开始需要为用户提供神经网络前向计算过程的服务，而用户希望数据中心能够尽快响应，因此，在数据中心部署一个满足低延迟前向过程的神经网络具有重要的意义。特别是随着神经网络结构变得更深更复杂，处理的应用也由2D变为3D，所需的计算量不断地在增长，低延迟就更加重要了。而现有的数据中心一般都是拥有大量的计算资源，这为加速神经网络前向计算过程提供了硬件基础。针对数据中心大量的硬件计算资源，如何发挥计算资源的效能并且满足用户的低延迟需求变得尤为重要。对于神经网络的训练来说，用户不会有低延迟的需求，但对于神经网络的前向推测过程，用户是非常关注延迟的。神经网络的训练中采用的并行方法对于神经网络的前向推测过程就不适用了。神经网络的训练是以批处理的方式进行训练的，一次处理大量的输入，为了减少计算过程间的通信，一般采用数据并行的方式。而神经网络的前向推测过程一般需要立即处理用户提交的单个输入，虽然有多个计算设备，但却无法采用训练中数据并行的方式进行加速，而需采用模型并行的方法才能达到低延迟的目标。模型并行是将模型参数分割在多个计算设备，这也可以解决模型参数过大在单个设备存储不足的问题，此外，神经网络每一层网络的计算都是由多个计算设备一起完成的，在每一层计算结束之后，各个计算设备间需要通信进行数据交换。

本章的目的主要是在数据中心环境下解决神经网络前向推测过程的低延迟问题。其中涉及到在多个计算设备上并行的问题，本章主要介绍如何采用模型并行的技术来对神经网络进行加速计算，并对模型并行中涉及到的通信问题，采用CUDA-Aware MPI技术使得GPU间通信更加高效，并且对多个计算设备间的通信模式进行了优化，使得通信代价不会随着计算设备的增多而增大。最后采用计算与通信进行重叠计算的技术进一步提升性能，这需要对模型并行进行流水化处理。

本章内容安排如下：

\section{相关研究}

\section{数据并行、模型并行以及CUDA-Aware MPI}

\section{模型并行的流水化}

\subsection{模型并行的流水化实现}


\subsection{优化的通信模式设计}


\subsection{计算与通信的重叠执行}

\section{实验评测与分析}

\subsection{实验设置}

\subsection{2D卷级层模型并行实现性能}

\subsection{3D卷级层模型并行实现性能}


\section{小结}
面对跟踪器日益复杂的结构和不断增加的计算负载，高性能的跟踪算法实现变得十分必要。
